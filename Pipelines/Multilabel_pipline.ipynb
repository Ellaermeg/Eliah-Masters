{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Data_processing')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score, precision_recall_curve, brier_score_loss, top_k_accuracy_score, balanced_accuracy_score, cohen_kappa_score, log_loss, PrecisionRecallDisplay\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_processing import KOProcessor \n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy import interpolate\n",
    "from itertools import cycle\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from scipy.stats import entropy\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADded changes for pipeline to run quicker, should equal 55 fits\n",
    " \n",
    "param_grid = [\n",
    "    {  \n",
    "        'estimator': [MultiOutputClassifier(RandomForestClassifier(random_state=42))],\n",
    "        'estimator__estimator__n_estimators': [100, 300],  # Reduced from [100, 200, 300]  \n",
    "        'estimator__estimator__max_depth': [5, None]  # Reduced from [5, 10, 15, None]\n",
    "    },\n",
    "    {  \n",
    "        'estimator': [MultiOutputClassifier(SVC(random_state=42, probability=True))],\n",
    "        'estimator__estimator__C': [0.1, 1],  # Reduced from [0.1, 1, 10]\n",
    "        'estimator__estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__estimator__gamma': ['scale']  # Removed 'auto'\n",
    "    },\n",
    "    {\n",
    "        'estimator': [MultiOutputClassifier(LogisticRegression())],\n",
    "        'estimator__estimator__C': [0.1, 1, 10]  # Reduced from [0.01, 0.1, 1, 10, 100]\n",
    "    }         \n",
    "]\n",
    "\n",
    "target_traits = ['oxygen']  # Add/remove traits as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "   Unnamed: 0   key      KO\n",
      "0           0  1000  K00001\n",
      "1           1  1000  K13954\n",
      "2           2  1000  K00003\n",
      "3           3  1000  K00013\n",
      "4           4  1000  K00014\n",
      "Data loaded successfully:\n",
      "   key        ID                      speciesStrain  \\\n",
      "0    1  592010.0   Abiotrophia defectiva ATCC 49176   \n",
      "1    1     219.0     Abiotrophia defectiva DSM 9849   \n",
      "2    2  159837.0       Abyssibacter profundi OUC007   \n",
      "3    3       NaN  Acanthopleuribacter pedis FYK2218   \n",
      "4    4  258515.0   Acetanaerobacterium elongatum Z7   \n",
      "\n",
      "                 speciesStrainComp                genus            genusComp  \\\n",
      "0    abiotrophiadefectivaatcc49176          Abiotrophia          abiotrophia   \n",
      "1      abiotrophiadefectivadsm9849          Abiotrophia          abiotrophia   \n",
      "2       abyssibacterprofundiouc007         Abyssibacter         abyssibacter   \n",
      "3  acanthopleuribacterpedisfyk2218  Acanthopleuribacter  acanthopleuribacter   \n",
      "4   acetanaerobacteriumelongatumz7  Acetanaerobacterium  acetanaerobacterium   \n",
      "\n",
      "                         species                   speciesComp      strain  \\\n",
      "0          Abiotrophia defectiva          abiotrophiadefectiva  ATCC 49176   \n",
      "1          Abiotrophia defectiva          abiotrophiadefectiva    DSM 9849   \n",
      "2          Abyssibacter profundi          abyssibacterprofundi      OUC007   \n",
      "3      Acanthopleuribacter pedis      acanthopleuribacterpedis     FYK2218   \n",
      "4  Acetanaerobacterium elongatum  acetanaerobacteriumelongatum          Z7   \n",
      "\n",
      "                              cultureNo typeStrain misclassified      gram  \\\n",
      "0                                   NaN        NaN           NaN  positive   \n",
      "1  DSM 9849|ATCC 49176|CIP 103242|SC 10        yes           NaN       NaN   \n",
      "2     KCTC 52933|MCCC 1K03450|JCM 32025        yes           NaN       NaN   \n",
      "3                                   NaN        NaN           NaN  negative   \n",
      "4                                   NaN        NaN           NaN  positive   \n",
      "\n",
      "        oxygen                                          substrate  \\\n",
      "0  facultative                                                NaN   \n",
      "1    anaerobic  alpha-cyclodextrin|arginine|D-arabitol|D-manni...   \n",
      "2      aerobic  2-oxopentanoate|4-hydroxybutyrate|adipate|algi...   \n",
      "3      aerobic  adipate|alanine|arginine|decanoate|D-glucose|D...   \n",
      "4    anaerobic  arabinose|cellobiose|esculin|fructose|galactos...   \n",
      "\n",
      "                                         genomeAccNo      usedAccNo  \\\n",
      "0  GCA_000160075|GCA_013267415|46125.34|46125.35|...  GCA_000160075   \n",
      "1  GCA_000160075|GCA_013267415|46125.34|46125.35|...  GCA_000160075   \n",
      "2    GCA_003151135|2831460164|QEQK00000000|2182787.3  GCA_003151135   \n",
      "3                             GCA_017377855|442870.4  GCA_017377855   \n",
      "4                 258515.18|GCA_900103835|2667527408  GCA_900103835   \n",
      "\n",
      "                                         ncbiFTP_FAA  \n",
      "0  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/0...  \n",
      "1  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/0...  \n",
      "2  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/0...  \n",
      "3  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/0...  \n",
      "4  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/9...  \n",
      "Aggregated labels for 'oxygen'. Unique keys: 3256\n",
      "\n",
      "Distribution for oxygen:\n",
      "oxygen\n",
      "0    1881\n",
      "2     498\n",
      "4     437\n",
      "5     264\n",
      "6     166\n",
      "3       6\n",
      "1       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "terms_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/terms_KO.zip'\n",
    "terms_csv_path = 'terms_KO.csv'\n",
    "traits_reduced_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/reducedDataset.zip'\n",
    "traits_reduced_csv_path = 'reducedDataset.csv'\n",
    "traits_assembled_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/assembledDataset.zip'\n",
    "traits_assembled_csv_path = 'assembledDataset.csv'\n",
    "\n",
    "processor = KOProcessor(\n",
    "    terms_zip_path, \n",
    "    terms_csv_path, \n",
    "    traits_reduced_zip_path, \n",
    "    traits_reduced_csv_path, \n",
    "    traits_assembled_zip_path=traits_assembled_zip_path, \n",
    "    traits_assembled_csv_path=traits_assembled_csv_path\n",
    ")\n",
    "\n",
    "ko_terms = processor.load_terms()\n",
    "if ko_terms is None:\n",
    "    raise FileNotFoundError(\"KO terms could not be loaded. Please check the file paths.\")\n",
    "\n",
    "reduced_traits_data = processor.load_reduced_traits_data()\n",
    "if reduced_traits_data is None:\n",
    "    raise FileNotFoundError(\"Reduced traits data could not be loaded. Please check the file paths.\")\n",
    "\n",
    "# Preprocess KO terms\n",
    "X_terms = processor.preprocess_terms(ko_terms)\n",
    "\n",
    "# Preprocess all target traits into a DataFrame\n",
    "y_dfs = []\n",
    "label_encoders = {}  # Store encoders for each trait\n",
    "\n",
    "for trait in target_traits:\n",
    "    y_trait = processor.preprocess_traits(reduced_traits_data, trait_column=trait, use_assembled_if_missing=True)\n",
    "    if y_trait is not None:\n",
    "        # Encode labels to numerical values\n",
    "        le = LabelEncoder()\n",
    "        encoded = le.fit_transform(y_trait)\n",
    "        y_dfs.append(pd.Series(encoded, index=y_trait.index, name=trait))\n",
    "        label_encoders[trait] = le\n",
    "\n",
    "y_combined = pd.concat(y_dfs, axis=1).dropna()\n",
    "\n",
    "# Align features with labels\n",
    "X_aligned, Y_aligned = processor.align_data(X_terms, y_combined)\n",
    "\n",
    "# Feature Selection: Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.02)\n",
    "X_aligned = selector.fit_transform(X_aligned)\n",
    "\n",
    "# Check trait distributions\n",
    "for trait in target_traits:\n",
    "    print(f\"\\nDistribution for {trait}:\")\n",
    "    print(Y_aligned[trait].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Modified Functions for Multi-Output\n",
    "#############################################\n",
    "\n",
    "def plot_advanced_confusion_matrix(y_true, y_pred, classes, title):\n",
    "    \"\"\"Enhanced confusion matrix with normalization and counts\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Normalized Confusion Matrix\\n{title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Add raw counts to bottom\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.set_xlim(ax.get_xlim())\n",
    "    ax2.set_xticks(ax.get_xticks())\n",
    "    ax2.set_xticklabels([str(int(x)) for x in cm.sum(axis=0)])\n",
    "    ax2.set_xlabel('Total Predictions')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(y_true, y_probs, classes, title):\n",
    "    \"\"\"Multiclass ROC curves with AUC scores\"\"\"\n",
    "    n_classes = len(classes)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all classes\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC {classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {title}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_probabilities(y_probs, classes, title):\n",
    "    \"\"\"Violin plot of class probability distributions\"\"\"\n",
    "    prob_df = pd.DataFrame(y_probs, columns=classes)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(data=prob_df, inner=\"quartile\", palette=\"Set3\")\n",
    "    plt.title(f'Class Probability Distributions\\n{title}')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(feature_importances, feature_names, top_n=20, title=\"\"):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    indices = np.argsort(feature_importances)[-top_n:]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(f'Top {top_n} Feature Importances\\n{title}')\n",
    "    plt.barh(range(top_n), feature_importances[indices], align='center')\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_errors(y_true, y_pred, classes, title):\n",
    "    \"\"\"Error analysis: which classes are confused with others\"\"\"\n",
    "    error_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.fill_diagonal(error_matrix, 0)  # Remove correct predictions\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(error_matrix, annot=True, fmt='d', cmap='Reds',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Prediction Errors\\n{title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def decode_labels(Y, label_encoders):\n",
    "    \"\"\"Convert numerical labels back to original text labels\"\"\"\n",
    "    decoded = Y.copy()\n",
    "    for trait in Y.columns:\n",
    "        decoded[trait] = label_encoders[trait].inverse_transform(Y[trait])\n",
    "    return decoded\n",
    "\n",
    "# =============================================\n",
    "#  Probability Trustworthiness Functions\n",
    "# =============================================\n",
    "\n",
    "def plot_calibration_curves(y_true, y_proba, classes, title):\n",
    "    \"\"\"Add this after existing plotting functions\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, class_name in enumerate(classes):\n",
    "        true_binary = (y_true == i).astype(int)\n",
    "        prob_pos = y_proba[:, i]\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            true_binary, prob_pos, n_bins=10, strategy='quantile'\n",
    "        )\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "                label=f'{class_name}')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k:', label='Perfect')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positive Samples')\n",
    "    plt.title(f'Calibration: {title}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_brier_scores(y_true, y_proba, classes):\n",
    "    \"\"\"Add this with other metric functions\"\"\"\n",
    "    scores = {}\n",
    "    for i, class_name in enumerate(classes):\n",
    "        true_binary = (y_true == i).astype(int)\n",
    "        scores[class_name] = brier_score_loss(true_binary, y_proba[:, i])\n",
    "    return pd.DataFrame.from_dict(scores, orient='index', columns=['Brier Score'])\n",
    "\n",
    "def analyze_prediction_entropy(y_proba, classes):\n",
    "    \"\"\"Enhanced with uncertainty samples identification\"\"\"\n",
    "    entropies = np.array([entropy(probs) for probs in y_proba])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.histplot(entropies, bins=30, kde=True)\n",
    "    \n",
    "    # Add uncertainty thresholds\n",
    "    low_uncertainty = np.log(len(classes)) * 0.3\n",
    "    high_uncertainty = np.log(len(classes)) * 0.7\n",
    "    ax.axvline(x=low_uncertainty, color='g', linestyle='--', label='Low uncertainty')\n",
    "    ax.axvline(x=high_uncertainty, color='r', linestyle='--', label='High uncertainty')\n",
    "    \n",
    "    plt.title('Prediction Entropy Distribution')\n",
    "    plt.xlabel('Entropy (Higher = More Uncertainty)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return indices of uncertain samples\n",
    "    return np.where(entropies > high_uncertainty)[0]\n",
    "\n",
    "def check_probability_consistency(y_proba, y_pred, classes):\n",
    "    \"\"\"Verify predicted class matches highest probability\"\"\"\n",
    "    mismatches = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if not np.isclose(y_proba[i, y_pred[i]], np.max(y_proba[i])):\n",
    "            mismatches += 1\n",
    "    print(f\"\\nProbability consistency: {100*(1-mismatches/len(y_pred)):.2f}% match\")\n",
    "\n",
    "def validate_probabilities(y_proba, y_pred):\n",
    "    \"\"\"Check probability validity with tolerance\"\"\"\n",
    "    # Check probability sums\n",
    "    if not np.allclose(y_proba.sum(axis=1), 1.0, atol=0.01):\n",
    "        print(\"Warning: Probabilities don't sum to 1 ±0.01\")\n",
    "    \n",
    "    # Check prediction consistency\n",
    "    mismatch_mask = y_pred != np.argmax(y_proba, axis=1)\n",
    "    mismatch_rate = np.mean(mismatch_mask)\n",
    "    \n",
    "    if mismatch_rate > 0:\n",
    "        print(f\"Warning: {mismatch_rate:.2%} predictions don't match max probabilities\")\n",
    "        print(\"Sample mismatches:\")\n",
    "        for i in np.where(mismatch_mask)[0][:3]:  # Show first 3\n",
    "            print(f\"Sample {i}: Predicted {y_pred[i]}, Max prob at {np.argmax(y_proba[i])}\")\n",
    "\n",
    "def plot_confidence_distribution(y_proba, title):\n",
    "    \"\"\"Distribution of maximum class probabilities\"\"\"\n",
    "    max_probs = np.max(y_proba, axis=1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(max_probs, bins=20, kde=True)\n",
    "    plt.title(f'Prediction Confidence: {title}')\n",
    "    plt.xlabel('Maximum Class Probability')\n",
    "    plt.show()\n",
    "\n",
    "def find_ambiguous_samples(y_proba, threshold=0.1):\n",
    "    \"\"\"Identify samples where top-2 classes are close\"\"\"\n",
    "    sorted_probs = np.sort(y_proba, axis=1)\n",
    "    return np.where(sorted_probs[:, -1] - sorted_probs[:, -2] < threshold)[0]\n",
    "\n",
    "def plot_metric_comparison(y_true, y_pred, y_proba, classes):\n",
    "    \"\"\"Comparative bar plot of key metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Cohen's Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        'Log Loss': log_loss(LabelEncoder().fit_transform(y_true), y_proba)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n",
    "    plt.title('Classifier Performance Metrics')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Score')\n",
    "    for i, v in enumerate(metrics.values()):\n",
    "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Per-Class Evaluation Functions\n",
    "# =============================================\n",
    "def plot_class_confusion_matrix(y_true, y_pred, class_name, class_idx):\n",
    "    \"\"\"Confusion matrix focused on one class\"\"\"\n",
    "    binary_true = (y_true == class_idx)\n",
    "    binary_pred = (y_pred == class_idx)\n",
    "    \n",
    "    cm = confusion_matrix(binary_true, binary_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Other', class_name],\n",
    "                yticklabels=['Other', class_name])\n",
    "    plt.title(f'Class Focus: {class_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_precision_recall(y_true, y_proba, class_idx, class_name):\n",
    "    \"\"\"Precision-Recall curve for individual classes\"\"\"\n",
    "    binary_true = (y_true == class_idx)\n",
    "    PrecisionRecallDisplay.from_predictions(binary_true, y_proba[:, class_idx])\n",
    "    plt.title(f'Precision-Recall: {class_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_feature_importance(importances, feature_names, class_name):\n",
    "    \"\"\"Feature importance for a specific class\"\"\"\n",
    "    indices = np.argsort(importances)[-20:]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(range(20), importances[indices], align='center')\n",
    "    plt.yticks(range(20), feature_names[indices])\n",
    "    plt.title(f'Top Features for {class_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "#  Model training and evaluation\n",
    "# =============================================\n",
    "\n",
    "def train_and_evaluate_multitrait(X_aligned, Y_aligned, target_traits, label_encoders, feature_names):\n",
    "    # Split data\n",
    "    print(f\"\\nStarting evaluation for traits: {target_traits}\")\n",
    "    print(f\"Total samples: {len(X_aligned)}, Features: {len(feature_names)}\")\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=0.2, random_state=42, stratify=Y_aligned)\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('estimator', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=4, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # After model training:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    print(cv_results[['param_estimator', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False))\n",
    "    print(\"\\nBest model parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    Y_proba_test = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Early probability check\n",
    "    print(\"\\nSample raw probability outputs:\")\n",
    "    for i, trait in enumerate(target_traits):\n",
    "        print(f\"\\nFirst 5 samples - {trait}:\")\n",
    "        print(pd.DataFrame(Y_proba_test[i][:5], columns=label_encoders[trait].classes_))\n",
    "    \n",
    "    # Decode labels\n",
    "    Y_test_decoded = decode_labels(pd.DataFrame(Y_test, columns=target_traits), label_encoders)\n",
    "    Y_pred_decoded = decode_labels(pd.DataFrame(Y_pred_test, columns=target_traits), label_encoders)\n",
    "    \n",
    "    # Per-trait evaluation\n",
    "    for idx, trait in enumerate(target_traits):\n",
    "        classes = label_encoders[trait].classes_\n",
    "        n_classes = len(classes)\n",
    "        y_true = Y_test_decoded[trait]\n",
    "        y_pred = Y_pred_decoded[trait]\n",
    "        y_proba = Y_proba_test[idx]\n",
    "        y_true_encoded = label_encoders[trait].transform(y_true)\n",
    "        \n",
    "        print(f\"\\n{'='*40}\\nEvaluation for {trait}\\n{'='*40}\")\n",
    "        print(f\"Expected classes: {list(classes)}\")\n",
    "        \n",
    "        # Validate probabilities first\n",
    "        validate_probabilities(y_proba, Y_pred_test[:, idx])\n",
    "        \n",
    "        # Core metrics comparison\n",
    "        plot_metric_comparison(y_true, y_pred, y_proba, classes)\n",
    "        \n",
    "        # 1. Classification Report (forced to include all classes)\n",
    "        print(\"\\nClassification Report (all classes):\")\n",
    "        print(classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=classes,\n",
    "            labels=np.arange(n_classes),\n",
    "            zero_division=0\n",
    "        ))\n",
    "\n",
    "        # 2. Classification Report (only present classes)\n",
    "        print(\"\\nClassification Report (present classes only):\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        # 3. Confidence Analysis\n",
    "        plot_confidence_distribution(y_proba, trait)\n",
    "        print(f\"Mean confidence: {np.max(y_proba, axis=1).mean():.2%}\")\n",
    "        \n",
    "        # 4. Top-2 Accuracy\n",
    "        if len(classes) > 2:\n",
    "            try:\n",
    "                top2_acc = top_k_accuracy_score(y_true_encoded, y_proba, k=2)\n",
    "                print(f\"Top-2 Accuracy: {top2_acc:.2%}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not compute top-2 accuracy: {str(e)}\")\n",
    "                \n",
    "        # 5. Ambiguous Samples\n",
    "        ambiguous_idx = find_ambiguous_samples(y_proba)\n",
    "        print(f\"Found {len(ambiguous_idx)} ambiguous samples (Δprob < 0.1)\")\n",
    "        \n",
    "        # 6. Advanced Confusion Matrix\n",
    "        plot_advanced_confusion_matrix(y_true, y_pred, classes, trait)\n",
    "        \n",
    "        # 7. ROC Curves (One-vs-Rest)\n",
    "        plot_roc_curves(y_true_encoded, y_proba, classes, trait)\n",
    "        \n",
    "        # 8. Class Probability Distributions\n",
    "        plot_class_probabilities(y_proba, classes, trait)\n",
    "        \n",
    "        # 9. Prediction Error Analysis\n",
    "        plot_prediction_errors(y_true, y_pred, classes, trait)\n",
    "        \n",
    "        # 10. Calibration Curves\n",
    "        plot_calibration_curves(y_true_encoded, y_proba, classes, trait)\n",
    "        \n",
    "        # 11. Brier Scores\n",
    "        brier_scores = calculate_brier_scores(y_true_encoded, y_proba, classes)\n",
    "        print(\"Brier Scores (Lower = Better):\")\n",
    "        print(brier_scores)\n",
    "        \n",
    "        # 12. Entropy Analysis\n",
    "        uncertain_samples = analyze_prediction_entropy(y_proba, classes)\n",
    "        print(f\"Found {len(uncertain_samples)} highly uncertain predictions\")\n",
    "        \n",
    "        \n",
    "        # 13. Feature Importance (for supported estimators)\n",
    "        try:\n",
    "            # Get the fitted estimator from the multi-output wrapper\n",
    "            fitted_estimator = best_model.named_steps['estimator'].estimators_[idx]\n",
    "            \n",
    "            if hasattr(fitted_estimator, 'feature_importances_'):\n",
    "                importances = fitted_estimator.feature_importances_\n",
    "                plot_feature_importance(importances, feature_names, title=trait)\n",
    "            else:\n",
    "                print(f\"\\nFeature importance not available for {type(fitted_estimator).__name__}\")\n",
    "        except (AttributeError, NotFittedError) as e:\n",
    "            print(f\"\\nCould not compute feature importance for {trait}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error computing feature importance: {str(e)}\")\n",
    "        \n",
    "        # 14. Per-label confusion matrix, precision-recall, and feature importance\n",
    "        for class_idx, class_name in enumerate(classes):\n",
    "            # Class-specific confusion matrix\n",
    "            plot_class_confusion_matrix(y_true_encoded, Y_pred_test[:, idx], class_name, class_idx)\n",
    "            \n",
    "            # Precision-Recall curve\n",
    "            plot_class_precision_recall(y_true_encoded, y_proba, class_idx, class_name)\n",
    "            \n",
    "            # Class-specific feature importance (if using RandomForest)\n",
    "            if 'randomforest' in str(best_model.named_steps['estimator'].estimator).lower():\n",
    "                importances = best_model.named_steps['estimator'].estimators_[idx].feature_importances_\n",
    "                plot_class_feature_importance(importances, feature_names, class_name)\n",
    "        \n",
    "        print(f\"\\nCompleted evaluation for {trait}\")\n",
    "    \n",
    "    # Return the best model after all traits and classes are processed\n",
    "    print(\"\\nEvaluation complete for all traits\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation for traits: ['oxygen']\n",
      "Total samples: 3256, Features: 5571\n",
      "Fitting 4 folds for each of 11 candidates, totalling 44 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the multiclass classification pipeline \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Results_model = \u001b[43mtrain_and_evaluate_multitrait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_aligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_aligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_traits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From variance threshold\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 253\u001b[39m, in \u001b[36mtrain_and_evaluate_multitrait\u001b[39m\u001b[34m(X_aligned, Y_aligned, target_traits, label_encoders, feature_names)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Grid Search\u001b[39;00m\n\u001b[32m    252\u001b[39m grid_search = GridSearchCV(pipeline, param_grid, cv=\u001b[32m4\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# After model training:\u001b[39;00m\n\u001b[32m    256\u001b[39m best_model = grid_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the multiclass classification pipeline \n",
    "Results_model = train_and_evaluate_multitrait(\n",
    "    X_aligned, \n",
    "    Y_aligned, \n",
    "    target_traits, \n",
    "    label_encoders,\n",
    "    feature_names=selector.get_feature_names_out()  # From variance threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts regarding the classification \n",
    "\n",
    "## 1. Advanced Confusion Matrix\n",
    "**What it shows**:  \n",
    "- Normalized prediction accuracy per class (rows sum to 1)  \n",
    "- Raw prediction counts in bottom axis  \n",
    "\n",
    "**Example**:  \n",
    "For oxygen tolerance prediction:  \n",
    "- 95% of true aerobes correctly predicted (row 1)  \n",
    "- 10% of anaerobes misclassified as facultative (row 2, column 3)  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies which microbial classes are frequently confused (e.g., facultative vs. microaerophilic)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ROC Curves & AUC Scores\n",
    "**What it shows**:  \n",
    "- True Positive Rate vs False Positive Rate for each class  \n",
    "- AUC = Area Under Curve (1.0 = perfect, 0.5 = random)  \n",
    "\n",
    "**Example**:  \n",
    "Facultative class AUC=0.92 vs Anaerobic AUC=0.85 suggests better distinction of facultative organisms  \n",
    "\n",
    "**Biological significance**:  \n",
    "Measures how well KO terms separate different metabolic strategies\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Class Probability Distributions\n",
    "**What it shows**:  \n",
    "- Distribution of predicted probabilities for each class  \n",
    "\n",
    "**Example**:  \n",
    "Narrow peak at 1.0 for \"Gram-positive\" shows high confidence  \n",
    "Wide distribution for \"Photolithotroph\" indicates uncertainty  \n",
    "\n",
    "**Biological significance**:  \n",
    "Reveals ambiguous cases that might represent transitional phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Importance\n",
    "**What it shows**:  \n",
    "- Top N most important KO terms for predictions  \n",
    "\n",
    "**Example**:  \n",
    "KO00010 (glycolysis) important for facultative prediction  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies key metabolic pathways associated with specific traits\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Prediction Error Analysis\n",
    "**What it shows**:  \n",
    "- Confusion between classes after removing correct predictions  \n",
    "\n",
    "**Example**:  \n",
    "Strong anaerobe↔facultative confusion suggests overlapping metabolic capabilities  \n",
    "\n",
    "**Biological significance**:  \n",
    "Highlights evolutionarily related traits that are hard to distinguish\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Calibration Curves\n",
    "**What it shows**:  \n",
    "- Alignment of predicted probabilities with actual frequencies  \n",
    "\n",
    "**Example**:  \n",
    "At 0.8 predicted probability, 75% are actually positive  \n",
    "\n",
    "**Biological significance**:  \n",
    "Indicates whether probabilities can be trusted for rare phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Brier Scores\n",
    "**What it shows**:  \n",
    "- Mean squared error of probabilities (0=perfect, 1=worst)  \n",
    "\n",
    "**Example**:  \n",
    "Brier=0.12 for aerobes vs 0.25 for microaerophiles  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies traits where genomic signatures are less distinct\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Prediction Entropy\n",
    "**What it shows**:  \n",
    "- Uncertainty distribution across predictions  \n",
    "- Thresholds: Low (<30% max), High (>70% max)  \n",
    "\n",
    "**Example**:  \n",
    "High entropy samples often match uncharacterized genomes  \n",
    "\n",
    "**Biological significance**:  \n",
    "Flags samples needing additional experimental validation\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Confidence Distribution\n",
    "**What it shows**:  \n",
    "- Distribution of maximum class probabilities  \n",
    "\n",
    "**Example**:  \n",
    "Peak at 0.6-0.8 suggests conservative predictions  \n",
    "\n",
    "**Biological significance**:  \n",
    "Indicates overall model confidence in genome annotations\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Ambiguous Samples\n",
    "**What it shows**:  \n",
    "- Samples where top-2 class probabilities differ by <0.1  \n",
    "\n",
    "**Example**:  \n",
    "Genome with 48% aerobic vs 45% facultative prediction  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies potentially novel or transitional phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Metric Comparison\n",
    "**What it shows**:  \n",
    "- Balanced Accuracy: Performance on imbalanced classes  \n",
    "- Cohen's Kappa: Agreement beyond chance  \n",
    "- Log Loss: Probability calibration quality  \n",
    "\n",
    "**Example**:  \n",
    "High Kappa (>0.8) but moderate Log Loss (0.4) indicates good predictions with overconfident probabilities  \n",
    "\n",
    "**Biological significance**:  \n",
    "Holistic performance assessment for trait prediction reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Questions to Answer:\n",
    "### Are probabilities calibrated?\n",
    "→ Check calibration curves & Brier scores\n",
    "\n",
    "### Where is the model uncertain?\n",
    "→ Entropy analysis + probability distributions\n",
    "\n",
    "### Do errors make biological sense?\n",
    "→ Confusion matrices between similar traits (e.g., facultative vs. microaerophilic)\n",
    "\n",
    "### Can we trust high-confidence predictions?\n",
    "→ Reliability diagrams for high-probability bins (>80%)\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "- Excellent agreement beyond chance (Kappa > 0.75)\n",
    "- Good handling of class imbalance (Balanced Acc > 0.8)\n",
    "- 92% of predictions include true label in top-2 guesses\n",
    "- Probability calibration is good (Log Loss < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "#  Individual Label Analysis Pipeline with Grid Search\n",
    "# =============================================\n",
    "\n",
    "def analyze_individual_labels(trait, X, y_combined, label_encoder, feature_names):\n",
    "    \"\"\"Run full pipeline with grid search for each individual label\"\"\"\n",
    "    classes = label_encoder.classes_\n",
    "    \n",
    "    # Modified parameter grid for binary classification\n",
    "    binary_param_grid = [\n",
    "        {\n",
    "            'classifier': [RandomForestClassifier(random_state=42)],\n",
    "            'classifier__n_estimators': [100, 300],\n",
    "            'classifier__max_depth': [5, None]\n",
    "        },\n",
    "        {\n",
    "            'classifier': [SVC(random_state=42, probability=True)],\n",
    "            'classifier__C': [0.1, 1],\n",
    "            'classifier__kernel': ['linear', 'rbf'],\n",
    "            'classifier__gamma': ['scale']\n",
    "        },\n",
    "        {\n",
    "            'classifier': [LogisticRegression()],\n",
    "            'classifier__C': [0.1, 1, 10]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for class_name in classes:\n",
    "        print(f\"\\n{'#'*40}\\nAnalyzing: {trait} - {class_name}\\n{'#'*40}\")\n",
    "        \n",
    "        # Create binary labels\n",
    "        y_binary = (y_combined[trait] == class_name).astype(int)\n",
    "        \n",
    "        # Split data with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    "        )\n",
    "        \n",
    "        # Pipeline with variance threshold\n",
    "        pipeline = Pipeline([\n",
    "            ('variance_threshold', VarianceThreshold(threshold=0.03)),\n",
    "            ('classifier', RandomForestClassifier())  # Default will be overridden by grid search\n",
    "        ])\n",
    "        \n",
    "        # Grid Search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            binary_param_grid,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"\\nBest parameters for {class_name}:\")\n",
    "        print(grid_search.best_params_)\n",
    "        \n",
    "        # Predict with best model\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix: {class_name}')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Confidence Distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(y_proba[y_test == 1], bins=20, kde=True, color='green', label='True Positives')\n",
    "        sns.histplot(y_proba[y_test == 0], bins=20, kde=True, color='red', label='True Negatives')\n",
    "        plt.title(f'Confidence Distribution: {class_name}')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Feature Importance (for tree-based models)\n",
    "        if 'randomforest' in str(best_model.named_steps['classifier']).lower():\n",
    "            importances = best_model.named_steps['classifier'].feature_importances_\n",
    "            indices = np.argsort(importances)[-20:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.title(f'Top Features for {class_name}')\n",
    "            plt.barh(range(20), importances[indices], align='center')\n",
    "            plt.yticks(range(20), feature_names[indices])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 4. Metrics Report\n",
    "        print(f\"\\nClassification Report for {class_name}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "        \n",
    "        # 5. Error Analysis\n",
    "        error_mask = (y_pred != y_test)\n",
    "        print(f\"\\nError Analysis ({error_mask.sum()} errors):\")\n",
    "        error_probs = y_proba[error_mask]\n",
    "        error_labels = y_test[error_mask]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(error_probs, bins=20, kde=True,\n",
    "                     hue=error_labels.map({0: 'False Positive', 1: 'False Negative'}))\n",
    "        plt.title(f'Error Probability Distribution: {class_name}')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage (Run in a new cell)\n",
    "trait_to_analyze = 'oxygen'\n",
    "analyze_individual_labels(\n",
    "    trait=trait_to_analyze,\n",
    "    X=X_aligned,\n",
    "    y_combined=y_combined,\n",
    "    label_encoder=label_encoders[trait_to_analyze],\n",
    "    feature_names=selector.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KEGG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pathways\n\u001b[32m     20\u001b[39m selected_important_features = X_terms.columns[selector.get_support()]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m pathways = \u001b[43mmap_ko_to_pathways\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_important_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Creating the adjacency matrix with translated KO terms, including original KO term\u001b[39;00m\n\u001b[32m     24\u001b[39m translated_kos = {ko: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTranslated_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mko\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ko \u001b[38;5;129;01min\u001b[39;00m selected_important_features}  \u001b[38;5;66;03m# Placeholder for actual translation function\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmap_ko_to_pathways\u001b[39m\u001b[34m(ko_terms)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_ko_to_pathways\u001b[39m(ko_terms):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     kegg = \u001b[43mKEGG\u001b[49m()\n\u001b[32m      4\u001b[39m     pathways = {}\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ko \u001b[38;5;129;01min\u001b[39;00m ko_terms:\n",
      "\u001b[31mNameError\u001b[39m: name 'KEGG' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# KEGG Pathway Mapping\n",
    "def map_ko_to_pathways(ko_terms):\n",
    "    kegg = KEGG()\n",
    "    pathways = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            gene_links = kegg.link(\"pathway\", ko)\n",
    "            if gene_links:\n",
    "                for entry in gene_links.strip().split(\"\\n\"):\n",
    "                    split_entry = entry.split(\"\\t\")\n",
    "                    if len(split_entry) >= 2:\n",
    "                        ko_id, pathway_id = split_entry[0], split_entry[1]\n",
    "                        if pathway_id not in pathways:\n",
    "                            pathways[pathway_id] = set()\n",
    "                        pathways[pathway_id].add(ko)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ko}: {e}\")\n",
    "    return pathways\n",
    "\n",
    "selected_important_features = X_terms.columns[selector.get_support()]\n",
    "pathways = map_ko_to_pathways(selected_important_features)\n",
    "\n",
    "# Creating the adjacency matrix with translated KO terms, including original KO term\n",
    "translated_kos = {ko: f\"Translated_{ko}\" for ko in selected_important_features}  # Placeholder for actual translation function\n",
    "pathway_matrix = pd.DataFrame(\n",
    "    index=[f\"{translated_kos[ko]} ({ko})\" for ko in selected_important_features],\n",
    "    columns=pathways.keys(),\n",
    "    data=0\n",
    ")\n",
    "for pathway, kos in pathways.items():\n",
    "    for ko in kos:\n",
    "        if ko in selected_important_features:\n",
    "            pathway_matrix.loc[f\"{translated_kos[ko]} ({ko})\", pathway] = 1\n",
    "\n",
    "# Fetch and rename pathway names for readability\n",
    "kegg = KEGG()\n",
    "for column in pathway_matrix.columns:\n",
    "    pathway_info = kegg.get(column)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else column\n",
    "    pathway_matrix.rename(columns={column: pathway_name}, inplace=True)\n",
    "\n",
    "print(\"Pathway matrix after renaming:\\n\", pathway_matrix)\n",
    "\n",
    "# Heatmap visualization\n",
    "sns.heatmap(pathway_matrix, annot=True, cmap=\"Greys\", cbar=False)\n",
    "plt.title(f'Adjacency Matrix of KO Terms and Pathways (Multilabel)')\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('KO Terms')\n",
    "plt.show()\n",
    "\n",
    "# Network Visualization\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define a list of general pathways to exclude\n",
    "excluded_pathways = [\"metabolic pathways\"]  # You can add more general terms here\n",
    "\n",
    "# Add nodes and edges with renamed pathway names\n",
    "for ko in selected_important_features:\n",
    "    translated_label = f\"{translated_kos[ko]} ({ko})\"\n",
    "    G.add_node(ko, title=translated_label, label=translated_label, color='red', size=20)\n",
    "\n",
    "for pathway_id, kos in pathways.items():\n",
    "    pathway_info = kegg.get(pathway_id)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else pathway_id\n",
    "    if pathway_name.lower() not in excluded_pathways:\n",
    "        G.add_node(pathway_name, title=pathway_name, label=pathway_name, color='blue', size=30)\n",
    "        for ko in kos:\n",
    "            G.add_edge(ko, pathway_name)\n",
    "\n",
    "# Pyvis network visualization\n",
    "nt = Network(\"800px\", \"1200px\", notebook=True, heading=f'Interactive Network of KO Terms and Pathways (Multilabel)', bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources='remote')\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "nt.save_graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
