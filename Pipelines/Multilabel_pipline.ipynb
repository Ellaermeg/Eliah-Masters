{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Data_processing')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score, precision_recall_curve, brier_score_loss, top_k_accuracy_score, balanced_accuracy_score, cohen_kappa_score, log_loss, PrecisionRecallDisplay, accuracy_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_processing import KOProcessor \n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy import interpolate\n",
    "from itertools import cycle\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from scipy.stats import entropy\n",
    "sns.set_palette('crest')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADded changes for pipeline to run quicker, should equal 55 fits\n",
    " \n",
    "param_grid = [\n",
    "    {  \n",
    "        'estimator': [MultiOutputClassifier(RandomForestClassifier(random_state=42))],\n",
    "        'estimator__estimator__n_estimators': [100, 300],  # Reduced from [100, 200, 300]  \n",
    "        'estimator__estimator__max_depth': [5, None]  # Reduced from [5, 10, 15, None]\n",
    "    },\n",
    "    {  \n",
    "        'estimator': [MultiOutputClassifier(SVC(random_state=42, probability=True))],\n",
    "        'estimator__estimator__C': [0.1, 1],  # Reduced from [0.1, 1, 10]\n",
    "        'estimator__estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__estimator__gamma': ['scale']  # Removed 'auto'\n",
    "    },\n",
    "    {\n",
    "        'estimator': [MultiOutputClassifier(LogisticRegression())],\n",
    "        'estimator__estimator__C': [0.1, 1, 10]  # Reduced from [0.01, 0.1, 1, 10, 100]\n",
    "    }         \n",
    "]\n",
    "\n",
    "target_traits = ['oxygen']  # Add/remove traits as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "terms_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/terms_KO.zip'\n",
    "terms_csv_path = 'terms_KO.csv'\n",
    "traits_reduced_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/reducedDataset.zip'\n",
    "traits_reduced_csv_path = 'reducedDataset.csv'\n",
    "traits_assembled_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/assembledDataset.zip'\n",
    "traits_assembled_csv_path = 'assembledDataset.csv'\n",
    "\n",
    "processor = KOProcessor(\n",
    "    terms_zip_path, \n",
    "    terms_csv_path, \n",
    "    traits_reduced_zip_path, \n",
    "    traits_reduced_csv_path, \n",
    "    traits_assembled_zip_path=traits_assembled_zip_path, \n",
    "    traits_assembled_csv_path=traits_assembled_csv_path\n",
    ")\n",
    "\n",
    "ko_terms = processor.load_terms()\n",
    "if ko_terms is None:\n",
    "    raise FileNotFoundError(\"KO terms could not be loaded. Please check the file paths.\")\n",
    "\n",
    "reduced_traits_data = processor.load_reduced_traits_data()\n",
    "if reduced_traits_data is None:\n",
    "    raise FileNotFoundError(\"Reduced traits data could not be loaded. Please check the file paths.\")\n",
    "\n",
    "# Preprocess KO terms\n",
    "X_terms = processor.preprocess_terms(ko_terms)\n",
    "\n",
    "# Preprocess all target traits into a DataFrame\n",
    "y_dfs = []\n",
    "label_encoders = {}  # Store encoders for each trait\n",
    "\n",
    "for trait in target_traits:\n",
    "    y_trait = processor.preprocess_traits(reduced_traits_data, trait_column=trait, use_assembled_if_missing=True)\n",
    "    if y_trait is not None:\n",
    "        # Encode labels to numerical values\n",
    "        le = LabelEncoder()\n",
    "        encoded = le.fit_transform(y_trait)\n",
    "        y_dfs.append(pd.Series(encoded, index=y_trait.index, name=trait))\n",
    "        label_encoders[trait] = le\n",
    "\n",
    "y_combined = pd.concat(y_dfs, axis=1).dropna()\n",
    "\n",
    "# Align features with labels\n",
    "X_aligned, Y_aligned = processor.align_data(X_terms, y_combined)\n",
    "\n",
    "# Feature Selection: Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.02)\n",
    "X_aligned = selector.fit_transform(X_aligned)\n",
    "\n",
    "# Check trait distributions\n",
    "for trait in target_traits:\n",
    "    print(f\"\\nDistribution for {trait}:\")\n",
    "    print(Y_aligned[trait].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Modified Functions for Multi-Output\n",
    "#############################################\n",
    "\n",
    "def plot_advanced_confusion_matrix(y_true, y_pred, classes, title):\n",
    "    \"\"\"Enhanced confusion matrix with normalization and counts\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='crest',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Normalized Confusion Matrix\\n{title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Add raw counts to bottom\n",
    "    ax2 = ax.twiny()\n",
    "    ax2.set_xlim(ax.get_xlim())\n",
    "    ax2.set_xticks(ax.get_xticks())\n",
    "    ax2.set_xticklabels([str(int(x)) for x in cm.sum(axis=0)])\n",
    "    ax2.set_xlabel('Total Predictions')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(y_true, y_probs, classes, title):\n",
    "    \"\"\"Multiclass ROC curves with AUC scores\"\"\"\n",
    "    n_classes = len(classes)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all classes\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(sns.color_palette('crest', n_classes))\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC {classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {title}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_probabilities(y_probs, classes, title):\n",
    "    \"\"\"Violin plot of class probability distributions\"\"\"\n",
    "    prob_df = pd.DataFrame(y_probs, columns=classes)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(data=prob_df, inner=\"quartile\", palette=\"Set3\")\n",
    "    plt.title(f'Class Probability Distributions\\n{title}')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(feature_importances, feature_names, top_n=20, title=\"\"):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    indices = np.argsort(feature_importances)[-top_n:]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(f'Top {top_n} Feature Importances\\n{title}')\n",
    "    plt.barh(range(top_n), feature_importances[indices], align='center')\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_errors(y_true, y_pred, classes, title):\n",
    "    \"\"\"Error analysis: which classes are confused with others\"\"\"\n",
    "    error_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.fill_diagonal(error_matrix, 0)  # Remove correct predictions\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(error_matrix, annot=True, fmt='d', cmap='crest',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Prediction Errors\\n{title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def decode_labels(Y, label_encoders):\n",
    "    \"\"\"Convert numerical labels back to original text labels\"\"\"\n",
    "    decoded = Y.copy()\n",
    "    for trait in Y.columns:\n",
    "        decoded[trait] = label_encoders[trait].inverse_transform(Y[trait])\n",
    "    return decoded\n",
    "\n",
    "# =============================================\n",
    "#  Probability Trustworthiness Functions\n",
    "# =============================================\n",
    "\n",
    "def plot_calibration_curves(y_true, y_proba, classes, title):\n",
    "    \"\"\"Add this after existing plotting functions\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, class_name in enumerate(classes):\n",
    "        true_binary = (y_true == i).astype(int)\n",
    "        prob_pos = y_proba[:, i]\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            true_binary, prob_pos, n_bins=10, strategy='quantile'\n",
    "        )\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "                label=f'{class_name}')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k:', label='Perfect')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positive Samples')\n",
    "    plt.title(f'Calibration: {title}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_brier_scores(y_true, y_proba, classes):\n",
    "    \"\"\"Add this with other metric functions\"\"\"\n",
    "    scores = {}\n",
    "    for i, class_name in enumerate(classes):\n",
    "        true_binary = (y_true == i).astype(int)\n",
    "        scores[class_name] = brier_score_loss(true_binary, y_proba[:, i])\n",
    "    return pd.DataFrame.from_dict(scores, orient='index', columns=['Brier Score'])\n",
    "\n",
    "def analyze_prediction_entropy(y_proba, classes):\n",
    "    \"\"\"Enhanced with uncertainty samples identification\"\"\"\n",
    "    entropies = np.array([entropy(probs) for probs in y_proba])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.histplot(entropies, bins=30, kde=True)\n",
    "    \n",
    "    # Add uncertainty thresholds\n",
    "    low_uncertainty = np.log(len(classes)) * 0.3\n",
    "    high_uncertainty = np.log(len(classes)) * 0.7\n",
    "    ax.axvline(x=low_uncertainty, color='g', linestyle='--', label='Low uncertainty')\n",
    "    ax.axvline(x=high_uncertainty, color='r', linestyle='--', label='High uncertainty')\n",
    "    \n",
    "    plt.title('Prediction Entropy Distribution')\n",
    "    plt.xlabel('Entropy (Higher = More Uncertainty)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return indices of uncertain samples\n",
    "    return np.where(entropies > high_uncertainty)[0]\n",
    "\n",
    "def check_probability_consistency(y_proba, y_pred, classes):\n",
    "    \"\"\"Verify predicted class matches highest probability\"\"\"\n",
    "    mismatches = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if not np.isclose(y_proba[i, y_pred[i]], np.max(y_proba[i])):\n",
    "            mismatches += 1\n",
    "    print(f\"\\nProbability consistency: {100*(1-mismatches/len(y_pred)):.2f}% match\")\n",
    "\n",
    "def validate_probabilities(y_proba, y_pred):\n",
    "    \"\"\"Check probability validity with tolerance\"\"\"\n",
    "    # Check probability sums\n",
    "    if not np.allclose(y_proba.sum(axis=1), 1.0, atol=0.01):\n",
    "        print(\"Warning: Probabilities don't sum to 1 ±0.01\")\n",
    "    \n",
    "    # Check prediction consistency\n",
    "    mismatch_mask = y_pred != np.argmax(y_proba, axis=1)\n",
    "    mismatch_rate = np.mean(mismatch_mask)\n",
    "    \n",
    "    if mismatch_rate > 0:\n",
    "        print(f\"Warning: {mismatch_rate:.2%} predictions don't match max probabilities\")\n",
    "        print(\"Sample mismatches:\")\n",
    "        for i in np.where(mismatch_mask)[0][:3]:  # Show first 3\n",
    "            print(f\"Sample {i}: Predicted {y_pred[i]}, Max prob at {np.argmax(y_proba[i])}\")\n",
    "\n",
    "def plot_confidence_distribution(y_proba, title):\n",
    "    \"\"\"Distribution of maximum class probabilities\"\"\"\n",
    "    max_probs = np.max(y_proba, axis=1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(max_probs, bins=20, kde=True)\n",
    "    plt.title(f'Prediction Confidence: {title}')\n",
    "    plt.xlabel('Maximum Class Probability')\n",
    "    plt.show()\n",
    "\n",
    "def find_ambiguous_samples(y_proba, threshold=0.1):\n",
    "    \"\"\"Identify samples where top-2 classes are close\"\"\"\n",
    "    sorted_probs = np.sort(y_proba, axis=1)\n",
    "    return np.where(sorted_probs[:, -1] - sorted_probs[:, -2] < threshold)[0]\n",
    "\n",
    "def plot_metric_comparison(y_true, y_pred, y_proba, classes):\n",
    "    \"\"\"Comparative bar plot of key metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Cohen's Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        'Log Loss': log_loss(LabelEncoder().fit_transform(y_true), y_proba)\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n",
    "    plt.title('Classifier Performance Metrics')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Score')\n",
    "    for i, v in enumerate(metrics.values()):\n",
    "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Per-Class Evaluation Functions\n",
    "# =============================================\n",
    "def plot_class_confusion_matrix(y_true, y_pred, class_name, class_idx):\n",
    "    \"\"\"Confusion matrix focused on one class\"\"\"\n",
    "    binary_true = (y_true == class_idx)\n",
    "    binary_pred = (y_pred == class_idx)\n",
    "    \n",
    "    cm = confusion_matrix(binary_true, binary_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='crest',\n",
    "                xticklabels=['Other', class_name],\n",
    "                yticklabels=['Other', class_name])\n",
    "    plt.title(f'Class Focus: {class_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_precision_recall(y_true, y_proba, class_idx, class_name):\n",
    "    \"\"\"Precision-Recall curve for individual classes\"\"\"\n",
    "    binary_true = (y_true == class_idx)\n",
    "    PrecisionRecallDisplay.from_predictions(binary_true, y_proba[:, class_idx])\n",
    "    plt.title(f'Precision-Recall: {class_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_feature_importance(importances, feature_names, class_name):\n",
    "    \"\"\"Feature importance for a specific class\"\"\"\n",
    "    indices = np.argsort(importances)[-20:]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(range(20), importances[indices], align='center')\n",
    "    plt.yticks(range(20), feature_names[indices])\n",
    "    plt.title(f'Top Features for {class_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "def plot_spider_metrics(y_true, y_pred, classes, title):\n",
    "    \"\"\"Radar chart of precision, recall, F1 and MCC per class.\"\"\"\n",
    "    from math import pi\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "    # compute for each class\n",
    "    metrics = {'precision': [], 'recall': [], 'f1': [], 'mcc': []}\n",
    "    for i, cls in enumerate(classes):\n",
    "        y_true_bin = (y_true == cls).astype(int)\n",
    "        y_pred_bin = (y_pred == cls).astype(int)\n",
    "        metrics['precision'].append(precision_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "        metrics['mcc'].append(matthews_corrcoef(y_true_bin, y_pred_bin))\n",
    "\n",
    "    # prepare radar\n",
    "    labels = list(metrics.keys())\n",
    "    N = len(labels)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(polar=True))\n",
    "    for idx, cls in enumerate(classes):\n",
    "        values = [metrics[m][idx] for m in labels]\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, lw=2, label=str(cls))\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(f'Metric Radar – {title}')\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "\n",
    "def plot_traits_spider(y_true, y_pred, traits, title):\n",
    "    \"\"\"\n",
    "    Radar chart showing macro‑F1 and MCC for each trait.\n",
    "    y_true, y_pred: arrays of shape (n_samples, n_traits)\n",
    "    traits: list of trait names in order\n",
    "    \"\"\"\n",
    "    from math import pi\n",
    "    from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "    # compute one macro‑F1 & MCC per trait\n",
    "    metrics = {'F1': [], 'MCC': []}\n",
    "    for idx in range(len(traits)):\n",
    "        y_t = y_true[:, idx]\n",
    "        y_p = y_pred[:, idx]\n",
    "        metrics['F1'].append(f1_score(y_t, y_p, average='macro', zero_division=0))\n",
    "        metrics['MCC'].append(matthews_corrcoef(y_t, y_p))\n",
    "\n",
    "    # prepare radar axes\n",
    "    labels = traits\n",
    "    N = len(labels)\n",
    "    angles = [n/float(N)*2*pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(polar=True))\n",
    "    for metric_name, vals in metrics.items():\n",
    "        vals_cycle = vals + vals[:1]\n",
    "        ax.plot(angles, vals_cycle, lw=2, label=metric_name)\n",
    "        ax.fill(angles, vals_cycle, alpha=0.25)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'All‑Traits Radar – {title}')\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1_vs_mcc(y_true, y_pred, classes, title):\n",
    "    \"\"\"Scatter F1 vs MCC per class.\"\"\"\n",
    "    from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "    f1s, mccs = [], []\n",
    "    for cls in classes:\n",
    "        y_true_bin = (y_true == cls).astype(int)\n",
    "        y_pred_bin = (y_pred == cls).astype(int)\n",
    "        f1s.append(f1_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "        mccs.append(matthews_corrcoef(y_true_bin, y_pred_bin))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.scatterplot(x=mccs, y=f1s)\n",
    "    for i, cls in enumerate(classes):\n",
    "        plt.text(mccs[i]+0.01, f1s[i]+0.01, str(cls))\n",
    "    plt.xlabel('MCC')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'F1 vs MCC – {title}')\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "#  Model training and evaluation\n",
    "# =============================================\n",
    "\n",
    "def train_and_evaluate_multitrait(X_aligned, Y_aligned, target_traits, label_encoders, feature_names):\n",
    "    # Split data\n",
    "    print(f\"\\nStarting evaluation for traits: {target_traits}\")\n",
    "    print(f\"Total samples: {len(X_aligned)}, Features: {len(feature_names)}\")\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=0.2, random_state=42, stratify=Y_aligned)\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('estimator', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=4, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # After model training:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    print(cv_results[['param_estimator', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False))\n",
    "    print(\"\\nBest model parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    Y_proba_test = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Early probability check\n",
    "    print(\"\\nSample raw probability outputs:\")\n",
    "    for i, trait in enumerate(target_traits):\n",
    "        print(f\"\\nFirst 5 samples - {trait}:\")\n",
    "        print(pd.DataFrame(Y_proba_test[i][:5], columns=label_encoders[trait].classes_))\n",
    "    \n",
    "    # Decode labels\n",
    "    Y_test_decoded = decode_labels(pd.DataFrame(Y_test, columns=target_traits), label_encoders)\n",
    "    Y_pred_decoded = decode_labels(pd.DataFrame(Y_pred_test, columns=target_traits), label_encoders)\n",
    "    \n",
    "    # Per-trait evaluation\n",
    "    for idx, trait in enumerate(target_traits):\n",
    "        classes = label_encoders[trait].classes_\n",
    "        n_classes = len(classes)\n",
    "        y_true = Y_test_decoded[trait]\n",
    "        y_pred = Y_pred_decoded[trait]\n",
    "        y_proba = Y_proba_test[idx]\n",
    "        y_true_encoded = label_encoders[trait].transform(y_true)\n",
    "        \n",
    "        print(f\"\\n{'='*40}\\nEvaluation for {trait}\\n{'='*40}\")\n",
    "        print(f\"Expected classes: {list(classes)}\")\n",
    "        \n",
    "        # Validate probabilities first\n",
    "        validate_probabilities(y_proba, Y_pred_test[:, idx])\n",
    "        \n",
    "        # Core metrics comparison\n",
    "        plot_metric_comparison(y_true, y_pred, y_proba, classes)\n",
    "\n",
    "        #spider plots\n",
    "        plot_spider_metrics(y_true_encoded, Y_pred_test[:, idx], classes, trait)\n",
    "\n",
    "        # NEW: F1 vs MCC scatter per class\n",
    "        plot_f1_vs_mcc(y_true_encoded, Y_pred_test[:, idx], classes, trait)\n",
    "        \n",
    "        # 1. Classification Report (forced to include all classes)\n",
    "        print(\"\\nClassification Report (all classes):\")\n",
    "        print(classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=classes,\n",
    "            labels=np.arange(n_classes),\n",
    "            zero_division=0\n",
    "        ))\n",
    "\n",
    "        \n",
    "        # 2. Classification Report (only present classes)\n",
    "        print(\"\\nClassification Report (present classes only):\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        # 3. Confidence Analysis\n",
    "        plot_confidence_distribution(y_proba, trait)\n",
    "        print(f\"Mean confidence: {np.max(y_proba, axis=1).mean():.2%}\")\n",
    "        \n",
    "        # 4. Top-2 Accuracy\n",
    "        if len(classes) > 2:\n",
    "            try:\n",
    "                top2_acc = top_k_accuracy_score(y_true_encoded, y_proba, k=2)\n",
    "                print(f\"Top-2 Accuracy: {top2_acc:.2%}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not compute top-2 accuracy: {str(e)}\")\n",
    "                \n",
    "        # 5. Ambiguous Samples\n",
    "        ambiguous_idx = find_ambiguous_samples(y_proba)\n",
    "        print(f\"Found {len(ambiguous_idx)} ambiguous samples (Δprob < 0.1)\")\n",
    "        \n",
    "        # 6. Advanced Confusion Matrix\n",
    "        plot_advanced_confusion_matrix(y_true, y_pred, classes, trait)\n",
    "        \n",
    "        # 7. ROC Curves (One-vs-Rest)\n",
    "        plot_roc_curves(y_true_encoded, y_proba, classes, trait)\n",
    "        \n",
    "        # 8. Class Probability Distributions\n",
    "        plot_class_probabilities(y_proba, classes, trait)\n",
    "        \n",
    "        # 9. Prediction Error Analysis\n",
    "        plot_prediction_errors(y_true, y_pred, classes, trait)\n",
    "        \n",
    "        # 10. Calibration Curves\n",
    "        plot_calibration_curves(y_true_encoded, y_proba, classes, trait)\n",
    "        \n",
    "        # 11. Brier Scores\n",
    "        brier_scores = calculate_brier_scores(y_true_encoded, y_proba, classes)\n",
    "        print(\"Brier Scores (Lower = Better):\")\n",
    "        print(brier_scores)\n",
    "        \n",
    "        # 12. Entropy Analysis\n",
    "        uncertain_samples = analyze_prediction_entropy(y_proba, classes)\n",
    "        print(f\"Found {len(uncertain_samples)} highly uncertain predictions\")\n",
    "        \n",
    "        \n",
    "        # 13. Feature Importance (for supported estimators)\n",
    "        try:\n",
    "            # Get the fitted estimator from the multi-output wrapper\n",
    "            fitted_estimator = best_model.named_steps['estimator'].estimators_[idx]\n",
    "            \n",
    "            if hasattr(fitted_estimator, 'feature_importances_'):\n",
    "                importances = fitted_estimator.feature_importances_\n",
    "                plot_feature_importance(importances, feature_names, title=trait)\n",
    "            else:\n",
    "                print(f\"\\nFeature importance not available for {type(fitted_estimator).__name__}\")\n",
    "        except (AttributeError, NotFittedError) as e:\n",
    "            print(f\"\\nCould not compute feature importance for {trait}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error computing feature importance: {str(e)}\")\n",
    "        \n",
    "        # 14. Per-label confusion matrix, precision-recall, and feature importance\n",
    "        for class_idx, class_name in enumerate(classes):\n",
    "            # Class-specific confusion matrix\n",
    "            plot_class_confusion_matrix(y_true_encoded, Y_pred_test[:, idx], class_name, class_idx)\n",
    "            \n",
    "            # Precision-Recall curve\n",
    "            plot_class_precision_recall(y_true_encoded, y_proba, class_idx, class_name)\n",
    "            \n",
    "            # Class-specific feature importance (if using RandomForest)\n",
    "            if 'randomforest' in str(best_model.named_steps['estimator'].estimator).lower():\n",
    "                importances = best_model.named_steps['estimator'].estimators_[idx].feature_importances_\n",
    "                plot_class_feature_importance(importances, feature_names, class_name)\n",
    "\n",
    "        plot_traits_spider(\n",
    "        Y_test.values,            # true labels array shape (n_samples, n_traits)\n",
    "        Y_pred_test,              # predicted labels same shape\n",
    "        target_traits,            # list of trait names\n",
    "        title=\"Summary\")\n",
    "\n",
    "        \n",
    "        print(f\"\\nCompleted evaluation for {trait}\")\n",
    "    \n",
    "    # Return the best model after all traits and classes are processed\n",
    "    print(\"\\nEvaluation complete for all traits\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multiclass classification pipeline \n",
    "Results_model = train_and_evaluate_multitrait(\n",
    "    X_aligned, \n",
    "    Y_aligned, \n",
    "    target_traits, \n",
    "    label_encoders,\n",
    "    feature_names=selector.get_feature_names_out()  # From variance threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts regarding the classification \n",
    "\n",
    "## 1. Advanced Confusion Matrix\n",
    "**What it shows**:  \n",
    "- Normalized prediction accuracy per class (rows sum to 1)  \n",
    "- Raw prediction counts in bottom axis  \n",
    "\n",
    "**Example**:  \n",
    "For oxygen tolerance prediction:  \n",
    "- 95% of true aerobes correctly predicted (row 1)  \n",
    "- 10% of anaerobes misclassified as facultative (row 2, column 3)  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies which microbial classes are frequently confused (e.g., facultative vs. microaerophilic)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ROC Curves & AUC Scores\n",
    "**What it shows**:  \n",
    "- True Positive Rate vs False Positive Rate for each class  \n",
    "- AUC = Area Under Curve (1.0 = perfect, 0.5 = random)  \n",
    "\n",
    "**Example**:  \n",
    "Facultative class AUC=0.92 vs Anaerobic AUC=0.85 suggests better distinction of facultative organisms  \n",
    "\n",
    "**Biological significance**:  \n",
    "Measures how well KO terms separate different metabolic strategies\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Class Probability Distributions\n",
    "**What it shows**:  \n",
    "- Distribution of predicted probabilities for each class  \n",
    "\n",
    "**Example**:  \n",
    "Narrow peak at 1.0 for \"Gram-positive\" shows high confidence  \n",
    "Wide distribution for \"Photolithotroph\" indicates uncertainty  \n",
    "\n",
    "**Biological significance**:  \n",
    "Reveals ambiguous cases that might represent transitional phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Importance\n",
    "**What it shows**:  \n",
    "- Top N most important KO terms for predictions  \n",
    "\n",
    "**Example**:  \n",
    "KO00010 (glycolysis) important for facultative prediction  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies key metabolic pathways associated with specific traits\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Prediction Error Analysis\n",
    "**What it shows**:  \n",
    "- Confusion between classes after removing correct predictions  \n",
    "\n",
    "**Example**:  \n",
    "Strong anaerobe↔facultative confusion suggests overlapping metabolic capabilities  \n",
    "\n",
    "**Biological significance**:  \n",
    "Highlights evolutionarily related traits that are hard to distinguish\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Calibration Curves\n",
    "**What it shows**:  \n",
    "- Alignment of predicted probabilities with actual frequencies  \n",
    "\n",
    "**Example**:  \n",
    "At 0.8 predicted probability, 75% are actually positive  \n",
    "\n",
    "**Biological significance**:  \n",
    "Indicates whether probabilities can be trusted for rare phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Brier Scores\n",
    "**What it shows**:  \n",
    "- Mean squared error of probabilities (0=perfect, 1=worst)  \n",
    "\n",
    "**Example**:  \n",
    "Brier=0.12 for aerobes vs 0.25 for microaerophiles  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies traits where genomic signatures are less distinct\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Prediction Entropy\n",
    "**What it shows**:  \n",
    "- Uncertainty distribution across predictions  \n",
    "- Thresholds: Low (<30% max), High (>70% max)  \n",
    "\n",
    "**Example**:  \n",
    "High entropy samples often match uncharacterized genomes  \n",
    "\n",
    "**Biological significance**:  \n",
    "Flags samples needing additional experimental validation\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Confidence Distribution\n",
    "**What it shows**:  \n",
    "- Distribution of maximum class probabilities  \n",
    "\n",
    "**Example**:  \n",
    "Peak at 0.6-0.8 suggests conservative predictions  \n",
    "\n",
    "**Biological significance**:  \n",
    "Indicates overall model confidence in genome annotations\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Ambiguous Samples\n",
    "**What it shows**:  \n",
    "- Samples where top-2 class probabilities differ by <0.1  \n",
    "\n",
    "**Example**:  \n",
    "Genome with 48% aerobic vs 45% facultative prediction  \n",
    "\n",
    "**Biological significance**:  \n",
    "Identifies potentially novel or transitional phenotypes\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Metric Comparison\n",
    "**What it shows**:  \n",
    "- Balanced Accuracy: Performance on imbalanced classes  \n",
    "- Cohen's Kappa: Agreement beyond chance  \n",
    "- Log Loss: Probability calibration quality  \n",
    "\n",
    "**Example**:  \n",
    "High Kappa (>0.8) but moderate Log Loss (0.4) indicates good predictions with overconfident probabilities  \n",
    "\n",
    "**Biological significance**:  \n",
    "Holistic performance assessment for trait prediction reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Questions to Answer:\n",
    "### Are probabilities calibrated?\n",
    "→ Check calibration curves & Brier scores\n",
    "\n",
    "### Where is the model uncertain?\n",
    "→ Entropy analysis + probability distributions\n",
    "\n",
    "### Do errors make biological sense?\n",
    "→ Confusion matrices between similar traits (e.g., facultative vs. microaerophilic)\n",
    "\n",
    "### Can we trust high-confidence predictions?\n",
    "→ Reliability diagrams for high-probability bins (>80%)\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "- Excellent agreement beyond chance (Kappa > 0.75)\n",
    "- Good handling of class imbalance (Balanced Acc > 0.8)\n",
    "- 92% of predictions include true label in top-2 guesses\n",
    "- Probability calibration is good (Log Loss < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "#  Individual Label Analysis Pipeline with Grid Search\n",
    "# =============================================\n",
    "\n",
    "def analyze_individual_labels(trait, X, y_combined, label_encoder, feature_names):\n",
    "    \"\"\"Run full pipeline with grid search for each individual label\"\"\"\n",
    "    classes = label_encoder.classes_\n",
    "    \n",
    "    # Modified parameter grid for binary classification\n",
    "    binary_param_grid = [\n",
    "        {\n",
    "            'classifier': [RandomForestClassifier(random_state=42)],\n",
    "            'classifier__n_estimators': [100, 300],\n",
    "            'classifier__max_depth': [5, None]\n",
    "        },\n",
    "        {\n",
    "            'classifier': [SVC(random_state=42, probability=True)],\n",
    "            'classifier__C': [0.1, 1],\n",
    "            'classifier__kernel': ['linear', 'rbf'],\n",
    "            'classifier__gamma': ['scale']\n",
    "        },\n",
    "        {\n",
    "            'classifier': [LogisticRegression()],\n",
    "            'classifier__C': [0.1, 1, 10]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for class_name in classes:\n",
    "        print(f\"\\n{'#'*40}\\nAnalyzing: {trait} - {class_name}\\n{'#'*40}\")\n",
    "        \n",
    "        # Create binary labels\n",
    "        y_binary = (y_combined[trait] == class_name).astype(int)\n",
    "        \n",
    "        # Split data with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    "        )\n",
    "        \n",
    "        # Pipeline with variance threshold\n",
    "        pipeline = Pipeline([\n",
    "            ('variance_threshold', VarianceThreshold(threshold=0.03)),\n",
    "            ('classifier', RandomForestClassifier())  # Default will be overridden by grid search\n",
    "        ])\n",
    "        \n",
    "        # Grid Search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            binary_param_grid,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"\\nBest parameters for {class_name}:\")\n",
    "        print(grid_search.best_params_)\n",
    "        \n",
    "        # Predict with best model\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='crest',\n",
    "                    xticklabels=['Negative', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix: {class_name}')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Confidence Distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(y_proba[y_test == 1], bins=20, kde=True, color='green', label='True Positives')\n",
    "        sns.histplot(y_proba[y_test == 0], bins=20, kde=True, color='red', label='True Negatives')\n",
    "        plt.title(f'Confidence Distribution: {class_name}')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Feature Importance (for tree-based models)\n",
    "        if 'randomforest' in str(best_model.named_steps['classifier']).lower():\n",
    "            importances = best_model.named_steps['classifier'].feature_importances_\n",
    "            indices = np.argsort(importances)[-20:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.title(f'Top Features for {class_name}')\n",
    "            plt.barh(range(20), importances[indices], align='center')\n",
    "            plt.yticks(range(20), feature_names[indices])\n",
    "            plt.xlabel('Relative Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 4. Metrics Report\n",
    "        print(f\"\\nClassification Report for {class_name}:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "        \n",
    "        # 5. Error Analysis\n",
    "        error_mask = (y_pred != y_test)\n",
    "        print(f\"\\nError Analysis ({error_mask.sum()} errors):\")\n",
    "        error_probs = y_proba[error_mask]\n",
    "        error_labels = y_test[error_mask]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(error_probs, bins=20, kde=True,\n",
    "                     hue=error_labels.map({0: 'False Positive', 1: 'False Negative'}))\n",
    "        plt.title(f'Error Probability Distribution: {class_name}')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage (Run in a new cell)\n",
    "trait_to_analyze = 'oxygen'\n",
    "analyze_individual_labels(\n",
    "    trait=trait_to_analyze,\n",
    "    X=X_aligned,\n",
    "    y_combined=y_combined,\n",
    "    label_encoder=label_encoders[trait_to_analyze],\n",
    "    feature_names=selector.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KEGG Pathway Mapping\n",
    "def map_ko_to_pathways(ko_terms):\n",
    "    kegg = KEGG()\n",
    "    pathways = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            gene_links = kegg.link(\"pathway\", ko)\n",
    "            if gene_links:\n",
    "                for entry in gene_links.strip().split(\"\\n\"):\n",
    "                    split_entry = entry.split(\"\\t\")\n",
    "                    if len(split_entry) >= 2:\n",
    "                        ko_id, pathway_id = split_entry[0], split_entry[1]\n",
    "                        if pathway_id not in pathways:\n",
    "                            pathways[pathway_id] = set()\n",
    "                        pathways[pathway_id].add(ko)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ko}: {e}\")\n",
    "    return pathways\n",
    "\n",
    "selected_important_features = X_terms.columns[selector.get_support()]\n",
    "pathways = map_ko_to_pathways(selected_important_features)\n",
    "\n",
    "# Creating the adjacency matrix with translated KO terms, including original KO term\n",
    "translated_kos = {ko: f\"Translated_{ko}\" for ko in selected_important_features}  # Placeholder for actual translation function\n",
    "pathway_matrix = pd.DataFrame(\n",
    "    index=[f\"{translated_kos[ko]} ({ko})\" for ko in selected_important_features],\n",
    "    columns=pathways.keys(),\n",
    "    data=0\n",
    ")\n",
    "for pathway, kos in pathways.items():\n",
    "    for ko in kos:\n",
    "        if ko in selected_important_features:\n",
    "            pathway_matrix.loc[f\"{translated_kos[ko]} ({ko})\", pathway] = 1\n",
    "\n",
    "# Fetch and rename pathway names for readability\n",
    "kegg = KEGG()\n",
    "for column in pathway_matrix.columns:\n",
    "    pathway_info = kegg.get(column)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else column\n",
    "    pathway_matrix.rename(columns={column: pathway_name}, inplace=True)\n",
    "\n",
    "print(\"Pathway matrix after renaming:\\n\", pathway_matrix)\n",
    "\n",
    "# Heatmap visualization\n",
    "sns.heatmap(pathway_matrix, annot=True, cmap=\"crest\", cbar=False)\n",
    "plt.title(f'Adjacency Matrix of KO Terms and Pathways (Multilabel)')\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('KO Terms')\n",
    "plt.show()\n",
    "\n",
    "# Network Visualization\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define a list of general pathways to exclude\n",
    "excluded_pathways = [\"metabolic pathways\"]  # You can add more general terms here\n",
    "\n",
    "# Add nodes and edges with renamed pathway names\n",
    "for ko in selected_important_features:\n",
    "    translated_label = f\"{translated_kos[ko]} ({ko})\"\n",
    "    G.add_node(ko, title=translated_label, label=translated_label, color='red', size=20)\n",
    "\n",
    "for pathway_id, kos in pathways.items():\n",
    "    pathway_info = kegg.get(pathway_id)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else pathway_id\n",
    "    if pathway_name.lower() not in excluded_pathways:\n",
    "        G.add_node(pathway_name, title=pathway_name, label=pathway_name, color='blue', size=30)\n",
    "        for ko in kos:\n",
    "            G.add_edge(ko, pathway_name)\n",
    "\n",
    "# Pyvis network visualization\n",
    "nt = Network(\"800px\", \"1200px\", notebook=True, heading=f'Interactive Network of KO Terms and Pathways (Multilabel)', bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources='remote')\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "nt.save_graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
