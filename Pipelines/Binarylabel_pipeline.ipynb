{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Data_processing')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, train_test_split, cross_val_predict, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bioservices import KEGG\n",
    "import networkx as nx\n",
    "from sklearn.utils import resample\n",
    "from pyvis.network import Network\n",
    "from data_processing import KOProcessor \n",
    "from scipy.stats import pearsonr\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Params\n",
    "estimator_configs = [\n",
    "    {\n",
    "        'name': 'RandomForest',\n",
    "        'params': {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [RandomForestClassifier(random_state=42)],\n",
    "            'estimator__n_estimators': [100, 200, 300],\n",
    "            'estimator__max_depth': [5, 10, 15, None]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVC',\n",
    "        'params': {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [SVC(random_state=42)],\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf'],\n",
    "            'estimator__gamma': ['scale', 'auto'],\n",
    "            'estimator__class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'LogisticRegression',\n",
    "        'params': {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [LogisticRegression()],\n",
    "            'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'BernoulliNB',\n",
    "        'params': {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [BernoulliNB()],\n",
    "            'estimator__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'estimator__binarize': [0.0]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [RandomForestClassifier(random_state=42)],\n",
    "        'estimator__n_estimators': [100, 200, 300],  \n",
    "        'estimator__max_depth': [5, 10, 15, None]  \n",
    "    },\n",
    "\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [SVC(random_state=42)],\n",
    "        'estimator__C': [0.1, 1, 10],  \n",
    "        'estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__gamma': ['scale', 'auto'],\n",
    "        'estimator__class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        'select_k__k' : [10, 50, 100, 200, 300, 500, 1000],\n",
    "        'estimator': [LogisticRegression()],\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },         \n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [BernoulliNB()],\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0, 10.0],  \n",
    "        'estimator__binarize': [0.0]  # Is automatically applied\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# For selecting which trait i want to predict\n",
    "\n",
    "\n",
    "\n",
    "# For selecting which trait to predict\n",
    "target_trait = \"gram\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "terms_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/terms_KO.zip'\n",
    "terms_csv_path = 'terms_KO.csv'\n",
    "traits_reduced_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/reducedDataset.zip'\n",
    "traits_reduced_csv_path = 'reducedDataset.csv'\n",
    "traits_assembled_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/assembledDataset.zip'\n",
    "traits_assembled_csv_path = 'assembledDataset.csv'\n",
    "\n",
    "processor = KOProcessor(\n",
    "    terms_zip_path, \n",
    "    terms_csv_path, \n",
    "    traits_reduced_zip_path, \n",
    "    traits_reduced_csv_path, \n",
    "    traits_assembled_zip_path=traits_assembled_zip_path, \n",
    "    traits_assembled_csv_path=traits_assembled_csv_path\n",
    ")\n",
    "\n",
    "ko_terms = processor.load_terms()\n",
    "if ko_terms is None:\n",
    "        raise FileNotFoundError(\"KO terms could not be loaded. Please check the file paths.\")\n",
    "\n",
    "reduced_traits_data = processor.load_reduced_traits_data()\n",
    "if reduced_traits_data is None:\n",
    "    raise FileNotFoundError(\"Reduced traits data could not be loaded. Please check the file paths.\")\n",
    "\n",
    "# Debug: Print columns of reduced_traits_data\n",
    "# print(\"Columns in reduced_traits_data:\", reduced_traits_data.columns.tolist())\n",
    "\n",
    "# Uses assembled dataset if data not in reduced\n",
    "traits_assembled = processor.load_assembled_traits_data()\n",
    "if traits_assembled is not None:\n",
    "    print(\"Columns in assembled_traits_data:\", traits_assembled.columns.tolist())\n",
    "\n",
    "#Feature_preprocess = processor.preprocess_features(ko_terms,reduced_traits_data)\n",
    "\n",
    "# Preprocess KO terms and traits\n",
    "X_terms = processor.preprocess_terms(ko_terms)\n",
    "y_traits = processor.preprocess_traits(reduced_traits_data, trait_column=target_trait, use_assembled_if_missing=True)\n",
    "\n",
    "# Check data validity and extract levels\n",
    "if y_traits is None:\n",
    "    raise ValueError(f\"Traits data for {target_trait} could not be processed\")\n",
    "\n",
    "# Dynamically get trait levels and validate binary classification\n",
    "trait_levels = y_traits.unique().tolist()\n",
    "if len(trait_levels) != 3:\n",
    "    raise ValueError(f\"Target trait '{target_trait}' is not binary. Found levels: {trait_levels}\")\n",
    "\n",
    "#####################################\n",
    "# Checking occurances of features\n",
    "\n",
    "trait_counts = y_traits.value_counts()\n",
    "print(f\"Counts of trait levels for {target_trait}:\")\n",
    "for level, count in trait_counts.items():\n",
    "    print(f\"{level}: {count}\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "trait_counts.plot(kind='bar')\n",
    "plt.title(f\"Counts of Each Level for {target_trait}\")\n",
    "plt.xlabel(\"Trait Level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "# Check if y_traits was processed correctly\n",
    "if y_traits is None:\n",
    "    raise ValueError(f\"Traits data for {target_trait} could not be processed. Please check the log for errors.\")\n",
    "\n",
    "# Align features and labels\n",
    "X_aligned, Y_aligned = processor.align_data(X_terms, y_traits)\n",
    "\n",
    "# Feature Selection: Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.02)\n",
    "X_aligned = selector.fit_transform(X_aligned)\n",
    "\n",
    "\n",
    "\n",
    "def plot_spider(metrics, title, palette_color=None):\n",
    "    \"\"\"\n",
    "    Draw a radar (spider) chart of these five metrics:\n",
    "      ['F1‑score','Accuracy','Specificity','Sensitivity','Precision']\n",
    "    `metrics`: dict with those exact keys mapping to floats in [0,1].\n",
    "    `palette_color`: a single color spec (e.g. seaborn palette color).\n",
    "    \"\"\"\n",
    "    # 1. What are the variable names?\n",
    "    categories = ['F1‑score','Accuracy','Specificity','Sensitivity','Precision', 'MCC']\n",
    "    N = len(categories)\n",
    "    # 2. Values and close the loop\n",
    "    values = [metrics[c] for c in categories]\n",
    "    values += values[:1]\n",
    "    # 3. Compute angle of each axis in the plot (in radians)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    # 4. Initialise spider plot\n",
    "    fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(polar=True))\n",
    "    # 5. Draw one axe per variable + add labels\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=12)\n",
    "    # 6. Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    yticks = [0.25, 0.50, 0.75, 1.00]\n",
    "    plt.yticks(yticks, [f\"{y:.2f}\" for y in yticks], color=\"grey\", size=10)\n",
    "    plt.ylim(0,1)\n",
    "    # 7. Plot data\n",
    "    if palette_color is None:\n",
    "        line_color = 'C0'\n",
    "        fill_color = 'C0'\n",
    "    else:\n",
    "        line_color = palette_color\n",
    "        fill_color = palette_color\n",
    "    ax.plot(angles, values, color=line_color, linewidth=2, linestyle='solid', label=title)\n",
    "    ax.fill(angles, values, color=fill_color, alpha=0.25)\n",
    "    # 8. Title\n",
    "    plt.title(title, size=14, y=1.1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFIED K-FOLD PIPELINE with multiple confusion matrixes\n",
    "#############################################\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Crest palette\n",
    "sns.set_palette('crest')\n",
    "cmap = 'crest'\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels, delimiter=\",\"):\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    for level in trait_levels:\n",
    "        col = f\"{trait_name}_{level}\"\n",
    "        binary_df[col] = trait_series.apply(lambda x: 1 if pd.notnull(x) and level in str(x).split(delimiter) else 0)\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False,\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap=cmap, cbar=False, ax=axes[0])\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap=cmap, cbar=False, ax=axes[1])\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({'Actual Yes': pd.Series(actual_counts),\n",
    "                             'Predicted Yes': pd.Series(predicted_counts)})\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6), colormap='crest')\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Radar Overlay\n",
    "#############################################\n",
    "\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels,\n",
    "                                     test_size=0.2, random_state=42):\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "\n",
    "    # split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned,\n",
    "        test_size=test_size, random_state=random_state,\n",
    "        stratify=Y_aligned)\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels)\n",
    "\n",
    "    best_models = {}\n",
    "    metrics_by_model = {}\n",
    "    palette = sns.color_palette('crest', n_colors=len(estimator_configs))\n",
    "\n",
    "    for idx, config in enumerate(estimator_configs):\n",
    "        name = config['name']\n",
    "        color = palette[idx]\n",
    "        logging.info(f\"Grid search for {name}\")\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif)),\n",
    "            ('estimator', config['params']['estimator'][0])\n",
    "        ])\n",
    "        gs = GridSearchCV(pipeline, config['params'],\n",
    "                          cv=StratifiedKFold(5, shuffle=True, random_state=random_state),\n",
    "                          n_jobs=-1, verbose=1)\n",
    "        gs.fit(X_train, Y_train)\n",
    "\n",
    "        logging.info(f\"Best params for {name}: {gs.best_params_}\")\n",
    "        logging.info(f\"Best CV score for {name}: {gs.best_score_:.3f}\")\n",
    "\n",
    "        model = gs.best_estimator_\n",
    "        best_models[name] = model\n",
    "\n",
    "        # test evaluation\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(Y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(Y_test, y_pred)\n",
    "        f1   = f1_score(Y_test, y_pred, average='macro')\n",
    "        acc  = accuracy_score(Y_test, y_pred)\n",
    "        prec = precision_score(Y_test, y_pred, average='macro')\n",
    "        sens = recall_score(Y_test, y_pred, average='macro')\n",
    "        tp = np.diag(cm)\n",
    "        fp = cm.sum(axis=0) - tp\n",
    "        fn = cm.sum(axis=1) - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        spec = np.mean(tn/(tn+fp))\n",
    "\n",
    "        metrics = {'F1-score':f1, 'Accuracy':acc, 'Specificity':spec,\n",
    "                   'Sensitivity':sens, 'Precision':prec, 'MCC':mcc}\n",
    "        metrics_by_model[name] = metrics\n",
    "        logging.info(f\"Metrics for {name}: {metrics}\")\n",
    "\n",
    "        # existing per-model plots\n",
    "        plot_confusion_matrix(cm, f\"{target_trait} - {name} (Overall)\", sorted(set(Y_test)|set(y_pred)))\n",
    "        for level in trait_levels:\n",
    "            y_true_b = [1 if y == level else 0 for y in Y_test]\n",
    "            y_pred_b = [1 if y == level else 0 for y in y_pred]\n",
    "            cm_b = confusion_matrix(y_true_b, y_pred_b)\n",
    "            plot_confusion_matrix(cm_b, f\"{target_trait} - {name} (Level='{level}')\",\n",
    "                                  [f\"Not {level}\", level])\n",
    "        pred_bin = create_binary_trait_table(pd.Series(y_pred, index=Y_test.index),\n",
    "                                             target_trait, trait_levels)\n",
    "        plot_side_by_side_comparison(actual_binary_test, pred_bin,\n",
    "                                     f\"{target_trait} (Test Set) - {name}\")\n",
    "        plot_distribution_comparison(actual_binary_test, pred_bin,\n",
    "                                     f\"{target_trait} (Test Set) - {name}\")\n",
    "\n",
    "    # --- single overlay spider chart for all models ---\n",
    "    categories = ['F1-score','Accuracy','Specificity','Sensitivity','Precision','MCC']\n",
    "    N = len(categories)\n",
    "    angles = [n/float(N)*2*np.pi for n in range(N)] + [0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8), subplot_kw=dict(polar=True))\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=12)\n",
    "    ax.set_rlabel_position(0)\n",
    "    yticks = [0.25,0.5,0.75,1.0]\n",
    "    plt.yticks(yticks, [f\"{y:.2f}\" for y in yticks], color='grey', size=10)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    for idx, (name, met) in enumerate(metrics_by_model.items()):\n",
    "        values = [met[c] for c in categories] + [met[categories[0]]]\n",
    "        color = palette[idx]\n",
    "        ax.plot(angles, values, label=name, color=color, linewidth=2)\n",
    "        ax.fill(angles, values, color=color, alpha=0.1)\n",
    "\n",
    "    plt.title(f\"Comparison of metrics by estimator for {target_trait}\", size=16, y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3,1.1))\n",
    "    plt.show()\n",
    "\n",
    "    return best_models, metrics_by_model\n",
    "\n",
    "# Example usage:\n",
    "best_models, all_metrics = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot analysis\n",
    "########################################################\n",
    "\n",
    "'''# --------------------------\n",
    "# Jaccard Index Stability Analysis-------------> Shows wich features are periodically selected for\n",
    "# --------------------------\n",
    "n_folds = 5\n",
    "selected_features = []\n",
    "skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "# Track selected features across folds\n",
    "for train_idx, _ in skf.split(X_aligned, Y_aligned):\n",
    "    selector = SelectKBest(f_classif, k=100)\n",
    "    selector.fit(X_aligned[train_idx], Y_aligned[train_idx])\n",
    "    # Convert feature_names to a numpy array for boolean indexing\n",
    "    selected_features.append(set(np.array(feature_names)[selector.get_support()]))\n",
    "\n",
    "# Compute Jaccard matrix\n",
    "jaccard_matrix = np.zeros((n_folds, n_folds))\n",
    "for i in range(n_folds):\n",
    "    for j in range(n_folds):\n",
    "        intersection = len(selected_features[i] & selected_features[j])\n",
    "        union = len(selected_features[i] | selected_features[j])\n",
    "        jaccard_matrix[i, j] = intersection / union\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(jaccard_matrix, annot=True, cmap=\"YlGnBu\", \n",
    "            xticklabels=range(1, n_folds+1), \n",
    "            yticklabels=range(1, n_folds+1))\n",
    "plt.title(\"Feature Stability Across CV Folds (Jaccard Index)\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Fold\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "'''# --------------------------\n",
    "# Ablation Study (Skip models without feature importance)\n",
    "# --------------------------\n",
    "if grid_search is not None:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    estimator_name = best_model.named_steps['estimator'].__class__.__name__\n",
    "    \n",
    "    # Skip models without feature importance/coefficients\n",
    "    if not (hasattr(best_model.named_steps['estimator'], 'feature_importances_') or \n",
    "            hasattr(best_model.named_steps['estimator'], 'coef_')):\n",
    "        print(f\"Skipping ablation for {estimator_name} (no feature importance available)\")\n",
    "    else:\n",
    "        # Get feature importances\n",
    "        if hasattr(best_model.named_steps['estimator'], 'feature_importances_'):\n",
    "            importances = best_model.named_steps['estimator'].feature_importances_\n",
    "        else:\n",
    "            importances = np.abs(best_model.named_steps['estimator'].coef_[0])\n",
    "        \n",
    "        # Remove top 10% features\n",
    "        n_features_to_remove = int(0.1 * len(importances))\n",
    "        top_features = np.argsort(importances)[-n_features_to_remove:]\n",
    "        \n",
    "        # Create ablated dataset\n",
    "        X_ablated = np.delete(X_aligned, top_features, axis=1)\n",
    "        \n",
    "        # Evaluate ablated performance\n",
    "        X_train_ablate, X_test_ablate, Y_train_ablate, Y_test_ablate = train_test_split(\n",
    "            X_ablated, Y_aligned, test_size=0.3, stratify=Y_aligned, random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model.fit(X_train_ablate, Y_train_ablate)\n",
    "        ablated_score = f1_score(Y_test_ablate, best_model.predict(X_test_ablate), average='macro')\n",
    "        \n",
    "        print(f\"Original F1: {grid_search.best_score_:.3f}\")  # Use grid_search's best_score_\n",
    "        print(f\"Ablated F1: {ablated_score:.3f}\")\n",
    "        print(f\"Features removed: {len(top_features)}/{X_aligned.shape[1]}\")\n",
    "else:\n",
    "    print(\"Error: grid_search not found.\")'''\n",
    "\n",
    "\n",
    "\n",
    "'''# --------------------------\n",
    "# SHAP Value Analysis\n",
    "# --------------------------\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.Explainer(grid_search.best_estimator_.named_steps['estimator'])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names, class_names=np.unique(Y_aligned))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFIED K-FOLD PIPELINE with multiple confusion matrices for best performing method\n",
    "#############################################\n",
    "# Configure logging to include timestamps and log levels\n",
    "#############################################\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#crest colormap for heatmaps\n",
    "sns.set_palette('crest')        # set default line and bar colors to crest\n",
    "cmap = sns.color_palette(\"crest\", as_cmap=True)  # create a colormap object\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels=None, delimiter=\",\"):\n",
    "    \"\"\"Dynamically creates binary columns based on provided or observed trait levels\"\"\"\n",
    "    if trait_levels is None:\n",
    "        trait_levels = trait_series.unique().tolist()\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    \n",
    "    for level in trait_levels:\n",
    "        col_name = f\"{trait_name}_{str(level).replace(' ', '_')}\"\n",
    "        binary_df[col_name] = trait_series.apply(\n",
    "            lambda x: 1 if str(level) in str(x).split(delimiter) else 0\n",
    "        )\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='crest', cbar=False,  # was 'Blues'\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap='crest', cbar=False, ax=axes[0])  # was 'YlGnBu'\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap='crest', cbar=False, ax=axes[1])  # was 'YlGnBu'\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Actual Yes': pd.Series(actual_counts),\n",
    "        'Predicted Yes': pd.Series(predicted_counts)\n",
    "    })\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6), colormap='crest')  # added colormap='crest'\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Independent Test Set Using StratifiedKFold\n",
    "#############################################\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels, test_size=0.2, random_state=42):\n",
    "    results = {}\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    \n",
    "    # Ensure Y_aligned is a pandas Series\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "    \n",
    "    # Split the data into training and test sets (using stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=test_size, random_state=random_state, stratify=Y_aligned\n",
    "    )\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n",
    "    \n",
    "    # Build Binary Table for Test Set (for later comparison)\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels=trait_levels)\n",
    "    \n",
    "    # Main ML Pipeline with Grid Search on Training Set\n",
    "    pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    # Use StratifiedKFold CV (instead of Leave-One-Out) on the training set for grid search\n",
    "    skf_train = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    logging.info(\"Starting grid search with StratifiedKFold CV on the training set.\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf_train, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    logging.info(f\"Best parameters for {target_trait}: {grid_search.best_params_}\")\n",
    "    logging.info(f\"Best CV score for {target_trait}: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the final model on the independent test set\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    mcc_test = matthews_corrcoef(Y_test, Y_pred_test)\n",
    "    f1_test = f1_score(Y_test, Y_pred_test, average='macro')\n",
    "    logging.info(f\"Test Set Evaluation - MCC: {mcc_test:.3f}, F1 Score: {f1_test:.3f}\")\n",
    "    \n",
    "    # Overall confusion matrix\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred_test)\n",
    "    class_labels = sorted(list(set(Y_test) | set(Y_pred_test)))  # Get all unique class labels\n",
    "    plot_confusion_matrix(cm_test, f\"{target_trait} Classifier (Test Set)\", class_labels)\n",
    "    \n",
    "    # One-vs-rest confusion matrices for each trait level\n",
    "    for level in trait_levels:\n",
    "        # Create binary labels: 1 if the trait equals the current level, else 0\n",
    "        y_true_binary = [1 if y == level else 0 for y in Y_test]\n",
    "        y_pred_binary = [1 if y == level else 0 for y in Y_pred_test]\n",
    "        cm_binary = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "        # Define binary class labels for clarity\n",
    "        binary_labels = [f\"Not {level}\", level]\n",
    "        plot_confusion_matrix(cm_binary, f\"{target_trait} - One-vs-Rest for '{level}'\", binary_labels)\n",
    "    \n",
    "    predicted_series_test = pd.Series(Y_pred_test, index=Y_test.index)\n",
    "    predicted_binary_test = create_binary_trait_table(predicted_series_test, target_trait, trait_levels)\n",
    "    \n",
    "    plot_side_by_side_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    plot_distribution_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the pipeline\n",
    "results = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# LEAVE ONE OUT PIPELINE\n",
    "#############################################\n",
    "\n",
    "# Configure logging to include timestamps and log levels\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels, delimiter=\",\"):\n",
    "    \"\"\"\n",
    "    Converts a trait Series into a binary DataFrame with one column per trait level.\n",
    "    For each sample, each column is assigned 1 (Yes) if that level is present and 0 (No) otherwise.\n",
    "    Works even if the trait contains multiple levels in a single string.\n",
    "    \"\"\"\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    for level in trait_levels:\n",
    "        col_name = f\"{trait_name}_{level}\"\n",
    "        def check_level(x):\n",
    "            if pd.isnull(x):\n",
    "                return 0\n",
    "            if isinstance(x, list):\n",
    "                return 1 if level in x else 0\n",
    "            parts = [p.strip() for p in str(x).split(delimiter)]\n",
    "            return 1 if level in parts else 0\n",
    "        binary_df[col_name] = trait_series.apply(check_level)\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    \"\"\"Plots a heatmap of the confusion matrix with correct labels.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Displays a side-by-side heatmap comparison of actual vs. predicted binary trait tables.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[0])\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[1])\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Creates a grouped bar chart comparing the distribution (Yes counts) of actual vs. predicted values.\"\"\"\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Actual Yes': pd.Series(actual_counts),\n",
    "        'Predicted Yes': pd.Series(predicted_counts)\n",
    "    })\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Independent Test Set Using LOO\n",
    "#############################################\n",
    "\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels, test_size=0.2, random_state=42):\n",
    "    results = {}\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    \n",
    "    # Ensure Y_aligned is a pandas Series\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "    \n",
    "    # Split the data into training and test sets (using stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=test_size, random_state=random_state, stratify=Y_aligned\n",
    "    )\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n",
    "    \n",
    "    # --- Build Binary Table for Test Set (for later comparison) ---\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels)\n",
    "    \n",
    "    # --- Main ML Pipeline with Grid Search on Training Set ---\n",
    "    pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier(random_state=random_state))\n",
    "    ])\n",
    "    \n",
    "    # Use Leave-One-Out CV on the training set for grid search\n",
    "    loo_train = LeaveOneOut()\n",
    "    logging.info(\"Starting grid search with Leave-One-Out CV on the training set.\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=loo_train, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    logging.info(f\"Best parameters for {target_trait}: {grid_search.best_params_}\")\n",
    "    logging.info(f\"Best CV score for {target_trait}: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # --- Evaluate the Final Model on the Independent Test Set ---\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    mcc_test = matthews_corrcoef(Y_test, Y_pred_test)\n",
    "    f1_test = f1_score(Y_test, Y_pred_test, average='macro')\n",
    "    logging.info(f\"Test Set Evaluation - MCC: {mcc_test:.3f}, F1 Score: {f1_test:.3f}\")\n",
    "    \n",
    "    # Plot test set evaluation results\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred_test)\n",
    "    class_labels = sorted(list(set(Y_test) | set(Y_pred_test)))  # Get all unique class labels\n",
    "    plot_confusion_matrix(cm_test, f\"{target_trait} Classifier (Test Set)\", class_labels)\n",
    "    \n",
    "    # Create binary table for predicted test labels and compare with actual\n",
    "    predicted_series_test = pd.Series(Y_pred_test, index=Y_test.index)\n",
    "    predicted_binary_test = create_binary_trait_table(predicted_series_test, target_trait, trait_levels)\n",
    "    \n",
    "    plot_side_by_side_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    plot_distribution_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "#######################\n",
    "# Trait selection\n",
    "#######################\n",
    "\n",
    "results = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# MCC vs. F1 Plot (Integrated with Cell 4 Pipeline)\n",
    "# --------------------------\n",
    "# Use the same train/test split as Cell 4\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_aligned, Y_aligned, test_size=0.3, stratify=Y_aligned, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare results dictionary\n",
    "results = {config['name']: {'f1': [], 'mcc': []} for config in estimator_configs}\n",
    "\n",
    "# Define k values to test\n",
    "k_values = range(1, 1000, 20)\n",
    "\n",
    "# StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for config in estimator_configs:\n",
    "    name = config['name']\n",
    "    estimator = config['params']['estimator'][0]\n",
    "    print(f\"Processing estimator: {name}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"  Testing with k = {k}\")\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif, k=k)),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "        \n",
    "        # F1 Score\n",
    "        f1_scores = cross_val_score(\n",
    "            pipeline, X_train, Y_train, cv=cv,\n",
    "            scoring=make_scorer(f1_score, average='macro'), n_jobs=-1\n",
    "        )\n",
    "        results[name]['f1'].append(f1_scores.mean())\n",
    "        \n",
    "        # MCC Score\n",
    "        mcc_scores = cross_val_score(\n",
    "            pipeline, X_train, Y_train, cv=cv,\n",
    "            scoring=make_scorer(matthews_corrcoef), n_jobs=-1\n",
    "        )\n",
    "        results[name]['mcc'].append(mcc_scores.mean())\n",
    "\n",
    "gfig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    k_values_list = list(k_values)\n",
    "    # F1 Score Plot\n",
    "    finite_f1_scores = [score for score in scores['f1'] if np.isfinite(score)]\n",
    "    finite_k_values_f1 = [k for k, score in zip(k_values_list, scores['f1']) if np.isfinite(score)]\n",
    "    ax[0].plot(finite_k_values_f1, finite_f1_scores, marker='o', linestyle='-', label=name)\n",
    "    # MCC Score Plot\n",
    "    finite_mcc_scores = [score for score in scores['mcc'] if np.isfinite(score)]\n",
    "    finite_k_values_mcc = [k for k, score in zip(k_values_list, scores['mcc']) if np.isfinite(score)]\n",
    "    ax[1].plot(finite_k_values_mcc, finite_mcc_scores, marker='o', linestyle='-', label=name)\n",
    "\n",
    "# Customize the F1 plot\n",
    "ax[0].set_title('F1 Score by Number of Selected Features (k) for Different Estimators (Training Set)')\n",
    "ax[0].set_xlabel('Number of Features (k)')\n",
    "ax[0].set_ylabel('F1 Score')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Customize the MCC plot\n",
    "ax[1].set_title('MCC Score by Number of Selected Features (k) for Different Estimators (Training Set)')\n",
    "ax[1].set_xlabel('Number of Features (k)')\n",
    "ax[1].set_ylabel('MCC Score')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pearson R for correlation coefficient between feature importance scores (From both models)'''\n",
    "\n",
    "lr_model = LogisticRegression(solver='lbfgs', penalty=\"l2\")\n",
    "rf_model = RandomForestClassifier(n_estimators=500, max_depth=100, min_samples_split=5, random_state=42)\n",
    "\n",
    "\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr, p_value = pearsonr(logistic_importance, rf_feature_importance)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr:.3f}\")\n",
    "#print(f\"P-Value: {p_value:.3f}\")\n",
    "\n",
    "# Bootstrap Pearson correlation coefficients\n",
    "n_bootstraps = 10000\n",
    "corr_coefficients = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Sample indices with replacement\n",
    "    indices = resample(np.arange(len(logistic_importance)))\n",
    "    \n",
    "    # Calculate Pearson correlation for the sample\n",
    "    r, _ = pearsonr(logistic_importance[indices], rf_feature_importance[indices])\n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=30, color='red', edgecolor='black')\n",
    "plt.title('Bootstrap Dist. of Pearson Correlation Coefficients')\n",
    "plt.xlabel('Pearson Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEGG Pathway Mapping\n",
    "def map_ko_to_pathways(ko_terms):\n",
    "    kegg = KEGG()\n",
    "    pathways = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            gene_links = kegg.link(\"pathway\", ko)\n",
    "            if gene_links:\n",
    "                for entry in gene_links.strip().split(\"\\n\"):\n",
    "                    split_entry = entry.split(\"\\t\")\n",
    "                    if len(split_entry) >= 2:\n",
    "                        ko_id, pathway_id = split_entry[0], split_entry[1]\n",
    "                        if pathway_id not in pathways:\n",
    "                            pathways[pathway_id] = set()\n",
    "                        pathways[pathway_id].add(ko)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ko}: {e}\")\n",
    "    return pathways\n",
    "\n",
    "selected_important_features = X_terms.columns[selector.get_support()]\n",
    "pathways = map_ko_to_pathways(selected_important_features)\n",
    "\n",
    "# Creating the adjacency matrix with translated KO terms, including original KO term\n",
    "translated_kos = {ko: f\"Translated_{ko}\" for ko in selected_important_features}  # Placeholder for actual translation function\n",
    "pathway_matrix = pd.DataFrame(\n",
    "    index=[f\"{translated_kos[ko]} ({ko})\" for ko in selected_important_features],\n",
    "    columns=pathways.keys(),\n",
    "    data=0\n",
    ")\n",
    "for pathway, kos in pathways.items():\n",
    "    for ko in kos:\n",
    "        if ko in selected_important_features:\n",
    "            pathway_matrix.loc[f\"{translated_kos[ko]} ({ko})\", pathway] = 1\n",
    "\n",
    "# Fetch and rename pathway names for readability\n",
    "kegg = KEGG()\n",
    "for column in pathway_matrix.columns:\n",
    "    pathway_info = kegg.get(column)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else column\n",
    "    pathway_matrix.rename(columns={column: pathway_name}, inplace=True)\n",
    "\n",
    "print(\"Pathway matrix after renaming:\\n\", pathway_matrix)\n",
    "\n",
    "# Heatmap visualization\n",
    "sns.heatmap(pathway_matrix, annot=True, cmap=\"Greys\", cbar=False)\n",
    "plt.title(f'Adjacency Matrix of KO Terms and Pathways ({target_trait})')\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('KO Terms')\n",
    "plt.show()\n",
    "\n",
    "# Network Visualization\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define a list of general pathways to exclude\n",
    "excluded_pathways = [\"metabolic pathways\"]  # You can add more general terms here\n",
    "\n",
    "# Add nodes and edges with renamed pathway names\n",
    "for ko in selected_important_features:\n",
    "    translated_label = f\"{translated_kos[ko]} ({ko})\"\n",
    "    G.add_node(ko, title=translated_label, label=translated_label, color='red', size=20)\n",
    "\n",
    "for pathway_id, kos in pathways.items():\n",
    "    pathway_info = kegg.get(pathway_id)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else pathway_id\n",
    "    if pathway_name.lower() not in excluded_pathways:\n",
    "        G.add_node(pathway_name, title=pathway_name, label=pathway_name, color='blue', size=30)\n",
    "        for ko in kos:\n",
    "            G.add_edge(ko, pathway_name)\n",
    "\n",
    "# Pyvis network visualization\n",
    "nt = Network(\"800px\", \"1200px\", notebook=True, heading=f'Interactive Network of KO Terms and Pathways ({target_trait})', bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources='remote')\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "nt.save_graph(f\"ko_network_{target_trait}.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
