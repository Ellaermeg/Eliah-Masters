{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Data_processing')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, train_test_split, cross_val_predict, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, make_scorer, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_processing import KOProcessor \n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from bioservices import KEGG\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import logging\n",
    "import seaborn\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "terms_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/terms_KO.zip'\n",
    "terms_csv_path = 'terms_KO.csv'\n",
    "traits_reduced_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/reducedDataset.zip'\n",
    "traits_reduced_csv_path = 'reducedDataset.csv'\n",
    "traits_assembled_zip_path = 'C:/Users/eliah/Documents/Master/Eliah-Masters/Datasets/assembledDataset.zip'\n",
    "traits_assembled_csv_path = 'assembledDataset.csv'\n",
    "\n",
    "processor = KOProcessor(\n",
    "    terms_zip_path, \n",
    "    terms_csv_path, \n",
    "    traits_reduced_zip_path, \n",
    "    traits_reduced_csv_path, \n",
    "    traits_assembled_zip_path=traits_assembled_zip_path, \n",
    "    traits_assembled_csv_path=traits_assembled_csv_path\n",
    ")\n",
    "\n",
    "# Load and preprocess KO terms and traits\n",
    "target_trait = \"gram\"\n",
    "\n",
    "ko_terms = processor.load_terms()\n",
    "if ko_terms is None:\n",
    "        raise FileNotFoundError(\"KO terms could not be loaded. Please check the file paths.\")\n",
    "\n",
    "reduced_traits_data = processor.load_reduced_traits_data()\n",
    "if reduced_traits_data is None:\n",
    "    raise FileNotFoundError(\"Reduced traits data could not be loaded. Please check the file paths.\")\n",
    "\n",
    "# Debug: Print columns of reduced_traits_data\n",
    "print(\"Columns in reduced_traits_data:\", reduced_traits_data.columns.tolist())\n",
    "\n",
    "# Uses assembled dataset if data not in reduced\n",
    "traits_assembled = processor.load_assembled_traits_data()\n",
    "if traits_assembled is not None:\n",
    "    print(\"Columns in assembled_traits_data:\", traits_assembled.columns.tolist())\n",
    "\n",
    "#Feature_preprocess = processor.preprocess_features(ko_terms,reduced_traits_data)\n",
    "\n",
    "# Preprocess KO terms and traits (trophy, gram, oxygen)\n",
    "X_terms = processor.preprocess_terms(ko_terms)\n",
    "y_traits = processor.preprocess_traits(reduced_traits_data, trait_column=target_trait, use_assembled_if_missing=True)\n",
    "\n",
    "# Check if y_traits was processed correctly\n",
    "if y_traits is None:\n",
    "    raise ValueError(f\"Traits data for {target_trait} could not be processed. Please check the log for errors.\")\n",
    "\n",
    "# Align features and labels\n",
    "X_aligned, Y_aligned = processor.align_data(X_terms, y_traits)\n",
    "\n",
    "# Feature Selection: Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_aligned = selector.fit_transform(X_aligned)\n",
    "\n",
    "# --------------------------\n",
    "# Volcano Plot Analysis -----------> identifies feature with high effect size (F-value) and significance\n",
    "# --------------------------\n",
    "# Get feature names after variance threshold\n",
    "feature_names = X_terms.columns[selector.get_support()]\n",
    "\n",
    "# Calculate ANOVA F-values and p-values\n",
    "f_values, p_values = f_classif(X_aligned, Y_aligned)\n",
    "\n",
    "# Create volcano plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(f_values, -np.log10(p_values), alpha=0.5)\n",
    "plt.xlabel('ANOVA F-value (Effect Size)')\n",
    "plt.ylabel('-log10(p-value)')\n",
    "plt.title(f'Volcano Plot: {target_trait} Feature Significance')\n",
    "plt.axhline(-np.log10(0.05), color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# STRATIFIED K-FOLD PIPELINE with multiple confusion matrixes\n",
    "#############################################\n",
    "\n",
    "# Configure logging to include timestamps and log levels\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels, delimiter=\",\"):\n",
    "    \"\"\"\n",
    "    Converts a trait Series into a binary DataFrame with one column per trait level.\n",
    "    For each sample, each column is assigned 1 (Yes) if that level is present and 0 (No) otherwise.\n",
    "    Works even if the trait contains multiple levels in a single string.\n",
    "    \"\"\"\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    for level in trait_levels:\n",
    "        col_name = f\"{trait_name}_{level}\"\n",
    "        def check_level(x):\n",
    "            if pd.isnull(x):\n",
    "                return 0\n",
    "            if isinstance(x, list):\n",
    "                return 1 if level in x else 0\n",
    "            parts = [p.strip() for p in str(x).split(delimiter)]\n",
    "            return 1 if level in parts else 0\n",
    "        binary_df[col_name] = trait_series.apply(check_level)\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        cbar=False, \n",
    "        xticklabels=class_labels, \n",
    "        yticklabels=class_labels\n",
    "    )\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Displays a side-by-side heatmap comparison of actual vs. predicted binary trait tables.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[0])\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[1])\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Creates a grouped bar chart comparing the distribution (Yes counts) of actual vs. predicted values.\"\"\"\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Actual Yes': pd.Series(actual_counts),\n",
    "        'Predicted Yes': pd.Series(predicted_counts)\n",
    "    })\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Independent Test Set Using StratifiedKFold\n",
    "#############################################\n",
    "\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels, test_size=0.3, random_state=42):\n",
    "    results = {}\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    \n",
    "    # Ensure Y_aligned is a pandas Series\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "    \n",
    "    # Split the data into training and test sets (using stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=test_size, random_state=random_state, stratify=Y_aligned\n",
    "    )\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n",
    "    \n",
    "    # Build Binary Table for Test Set (for later comparison)\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels)\n",
    "    \n",
    "    # Main ML Pipeline with Grid Search on Training Set\n",
    "    pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    # Define estimators and their parameter grids separately\n",
    "    estimator_configs = [\n",
    "        {\n",
    "            'name': 'RandomForest',\n",
    "            'params': {\n",
    "                'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "                'estimator': [RandomForestClassifier(random_state=42)],\n",
    "                'estimator__n_estimators': [100, 200, 300],\n",
    "                'estimator__max_depth': [5, 10, 15, None]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'SVC',\n",
    "            'params': {\n",
    "                'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "                'estimator': [SVC(random_state=42)],\n",
    "                'estimator__C': [0.1, 1, 10],\n",
    "                'estimator__kernel': ['linear', 'rbf'],\n",
    "                'estimator__gamma': ['scale', 'auto'],\n",
    "                'estimator__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'LogisticRegression',\n",
    "            'params': {\n",
    "                'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "                'estimator': [LogisticRegression()],\n",
    "                'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'BernoulliNB',\n",
    "            'params': {\n",
    "                'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "                'estimator': [BernoulliNB()],\n",
    "                'estimator__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "                'estimator__binarize': [0.0]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    best_models = {}\n",
    "    for config in estimator_configs:\n",
    "        logging.info(f\"Grid search for {config['name']}...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif)),\n",
    "            ('estimator', config['params']['estimator'][0])\n",
    "        ])\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            config['params'],\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        best_models[config['name']] = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate all best models\n",
    "    results = {}\n",
    "    for model_name, model in best_models.items():\n",
    "        Y_pred_test = model.predict(X_test)\n",
    "        mcc_test = matthews_corrcoef(Y_test, Y_pred_test)\n",
    "        f1_test = f1_score(Y_test, Y_pred_test, average='macro')\n",
    "        results[model_name] = {'MCC': mcc_test, 'F1': f1_test}\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(Y_test, Y_pred_test)\n",
    "        class_labels = sorted(list(set(Y_test) | set(Y_pred_test)))\n",
    "        plot_confusion_matrix(cm, f\"{target_trait} - {model_name}\", class_labels)\n",
    "\n",
    "        predicted_series_test = pd.Series(Y_pred_test, index=Y_test.index)\n",
    "        predicted_binary_test = create_binary_trait_table(predicted_series_test, target_trait, trait_levels)\n",
    "        \n",
    "        plot_side_by_side_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "        plot_distribution_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Oxygen:\n",
    "#target_trait = \"oxygen\"\n",
    "#trait_levels = ['aerobic', 'aerotolerant', 'microaerophilic', 'obligate_aerobic', 'anaerobic', 'obligate_anaerobic', 'conflict', 'facultative']\n",
    "\n",
    "# Gramstain:\n",
    "target_trait = \"gram\"\n",
    "trait_levels = ['positive', 'negative']\n",
    "\n",
    "# Trophy:\n",
    "#target_trait = \"trophy\"\n",
    "#trait_levels = ['photo', 'chemo', 'litho', 'hetero', 'organo', 'auto']\n",
    "results = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Plot analysis\n",
    "########################################################\n",
    "\n",
    "# --------------------------\n",
    "# Jaccard Index Stability Analysis -------------> Shows wich features are periodically selected for\n",
    "# --------------------------\n",
    "# Initialize variables\n",
    "n_folds = 5\n",
    "selected_features = []\n",
    "skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "# Track selected features across folds\n",
    "for train_idx, _ in skf.split(X_aligned, Y_aligned):\n",
    "    selector = SelectKBest(f_classif, k=100)\n",
    "    selector.fit(X_aligned[train_idx], Y_aligned[train_idx])\n",
    "    selected_features.append(set(feature_names[selector.get_support()]))\n",
    "\n",
    "# Compute Jaccard matrix\n",
    "jaccard_matrix = np.zeros((n_folds, n_folds))\n",
    "for i in range(n_folds):\n",
    "    for j in range(n_folds):\n",
    "        intersection = len(selected_features[i] & selected_features[j])\n",
    "        union = len(selected_features[i] | selected_features[j])\n",
    "        jaccard_matrix[i, j] = intersection / union\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(jaccard_matrix, annot=True, cmap=\"YlGnBu\", \n",
    "            xticklabels=range(1, n_folds+1), \n",
    "            yticklabels=range(1, n_folds+1))\n",
    "plt.title(\"Feature Stability Across CV Folds (Jaccard Index)\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Fold\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Ablation Study -----------------> removes top features to check for importance\n",
    "# --------------------------\n",
    "# Get best model from previous analysis\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Get feature importances\n",
    "if hasattr(best_model.named_steps['estimator'], 'feature_importances_'):\n",
    "    importances = best_model.named_steps['estimator'].feature_importances_\n",
    "else:  # For linear models\n",
    "    importances = np.abs(best_model.named_steps['estimator'].coef_[0])\n",
    "\n",
    "# Remove top 10% features\n",
    "n_features_to_remove = int(0.1 * len(importances))\n",
    "top_features = np.argsort(importances)[-n_features_to_remove:]\n",
    "\n",
    "# Create ablated dataset\n",
    "X_ablated = np.delete(X_aligned, top_features, axis=1)\n",
    "\n",
    "# Evaluate ablated performance\n",
    "X_train_ablate, X_test_ablate, Y_train_ablate, Y_test_ablate = train_test_split(\n",
    "    X_ablated, Y_aligned, test_size=0.3, stratify=Y_aligned\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_ablate, Y_train_ablate)\n",
    "ablated_score = f1_score(Y_test_ablate, best_model.predict(X_test_ablate), average='macro')\n",
    "\n",
    "print(f\"Original F1: {results[grid_search.best_params_['estimator'].__class__.__name__]['F1']:.3f}\")\n",
    "print(f\"Ablated F1: {ablated_score:.3f}\")\n",
    "print(f\"Features removed: {len(top_features)}/{X_aligned.shape[1]} ({len(top_features)/X_aligned.shape[1]:.1%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# SHAP Value Analysis\n",
    "# --------------------------\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.Explainer(grid_search.best_estimator_.named_steps['estimator'])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names, class_names=np.unique(Y_aligned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFIED K-FOLD PIPELINE single confusion matrix for best performing method\n",
    "#############################################\n",
    "\n",
    "# Configure logging to include timestamps and log levels\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels, delimiter=\",\"):\n",
    "    \"\"\"\n",
    "    Converts a trait Series into a binary DataFrame with one column per trait level.\n",
    "    For each sample, each column is assigned 1 (Yes) if that level is present and 0 (No) otherwise.\n",
    "    Works even if the trait contains multiple levels in a single string.\n",
    "    \"\"\"\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    for level in trait_levels:\n",
    "        col_name = f\"{trait_name}_{level}\"\n",
    "        def check_level(x):\n",
    "            if pd.isnull(x):\n",
    "                return 0\n",
    "            if isinstance(x, list):\n",
    "                return 1 if level in x else 0\n",
    "            parts = [p.strip() for p in str(x).split(delimiter)]\n",
    "            return 1 if level in parts else 0\n",
    "        binary_df[col_name] = trait_series.apply(check_level)\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    \"\"\"Plots a heatmap of the confusion matrix with correct labels.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Displays a side-by-side heatmap comparison of actual vs. predicted binary trait tables.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[0])\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[1])\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Creates a grouped bar chart comparing the distribution (Yes counts) of actual vs. predicted values.\"\"\"\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Actual Yes': pd.Series(actual_counts),\n",
    "        'Predicted Yes': pd.Series(predicted_counts)\n",
    "    })\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Independent Test Set Using StratifiedKFold\n",
    "#############################################\n",
    "\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels, test_size=0.3, random_state=42):\n",
    "    results = {}\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    \n",
    "    # Ensure Y_aligned is a pandas Series\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "    \n",
    "    # Split the data into training and test sets (using stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=test_size, random_state=random_state, stratify=Y_aligned\n",
    "    )\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n",
    "    \n",
    "    # Build Binary Table for Test Set (for later comparison)\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels)\n",
    "    \n",
    "    # Main ML Pipeline with Grid Search on Training Set\n",
    "    pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    param_grid = [\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [RandomForestClassifier(random_state=42)],\n",
    "        'estimator__n_estimators': [100, 200, 300],  \n",
    "        'estimator__max_depth': [5, 10, 15, None]  \n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [SVC(random_state=42)],\n",
    "        'estimator__C': [0.1, 1, 10],  \n",
    "        'estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__gamma': ['scale', 'auto'],\n",
    "        'estimator__class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        'select_k__k' : [10, 50, 100, 200, 300, 500, 1000],\n",
    "        'estimator': [LogisticRegression()],\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },         \n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [BernoulliNB()],\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0, 10.0],  \n",
    "        'estimator__binarize': [0.0]  # Is automatically applied\n",
    "    }\n",
    "]\n",
    "    \n",
    "    # Use StratifiedKFold CV (instead of Leave-One-Out) on the training set for grid search\n",
    "    skf_train = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    logging.info(\"Starting grid search with StratifiedKFold CV on the training set.\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=skf_train, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    logging.info(f\"Best parameters for {target_trait}: {grid_search.best_params_}\")\n",
    "    logging.info(f\"Best CV score for {target_trait}: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the final model on the independent test set\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    mcc_test = matthews_corrcoef(Y_test, Y_pred_test)\n",
    "    f1_test = f1_score(Y_test, Y_pred_test, average='macro')\n",
    "    logging.info(f\"Test Set Evaluation - MCC: {mcc_test:.3f}, F1 Score: {f1_test:.3f}\")\n",
    "    \n",
    "    # Plot test set evaluation results\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred_test)\n",
    "    class_labels = sorted(list(set(Y_test) | set(Y_pred_test)))  # Get all unique class labels\n",
    "    plot_confusion_matrix(cm_test, f\"{target_trait} Classifier (Test Set)\", class_labels)\n",
    "\n",
    "    \n",
    "    predicted_series_test = pd.Series(Y_pred_test, index=Y_test.index)\n",
    "    predicted_binary_test = create_binary_trait_table(predicted_series_test, target_trait, trait_levels)\n",
    "    \n",
    "    plot_side_by_side_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    plot_distribution_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "########################################\n",
    "# Trait selection\n",
    "########################################\n",
    "\n",
    "# Oxygen:\n",
    "#target_trait = \"oxygen\"\n",
    "#trait_levels = ['aerobic', 'aerotolerant', 'microaerophilic', 'obligate_aerobic', 'anaerobic', 'obligate_anaerobic', 'conflict', 'facultative']\n",
    "\n",
    "# Gramstain:\n",
    "#target_trait = \"gram\"\n",
    "#trait_levels = ['positive', 'negative']\n",
    "\n",
    "# Trophy:\n",
    "#target_trait = \"trophy\"\n",
    "#trait_levels = ['photo', 'chemo', 'litho', 'hetero', 'organo', 'auto']\n",
    "\n",
    "results = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE ONE OUT PIPELINE\n",
    "#############################################\n",
    "\n",
    "# Configure logging to include timestamps and log levels\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#############################################\n",
    "# Helper Function: Binary Table Creation\n",
    "#############################################\n",
    "\n",
    "def create_binary_trait_table(trait_series, trait_name, trait_levels, delimiter=\",\"):\n",
    "    \"\"\"\n",
    "    Converts a trait Series into a binary DataFrame with one column per trait level.\n",
    "    For each sample, each column is assigned 1 (Yes) if that level is present and 0 (No) otherwise.\n",
    "    Works even if the trait contains multiple levels in a single string.\n",
    "    \"\"\"\n",
    "    binary_df = pd.DataFrame(index=trait_series.index)\n",
    "    for level in trait_levels:\n",
    "        col_name = f\"{trait_name}_{level}\"\n",
    "        def check_level(x):\n",
    "            if pd.isnull(x):\n",
    "                return 0\n",
    "            if isinstance(x, list):\n",
    "                return 1 if level in x else 0\n",
    "            parts = [p.strip() for p in str(x).split(delimiter)]\n",
    "            return 1 if level in parts else 0\n",
    "        binary_df[col_name] = trait_series.apply(check_level)\n",
    "    return binary_df\n",
    "\n",
    "#############################################\n",
    "# Post-Prediction Visualization Functions\n",
    "#############################################\n",
    "\n",
    "def plot_confusion_matrix(cm, title, class_labels):\n",
    "    \"\"\"Plots a heatmap of the confusion matrix with correct labels.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Displays a side-by-side heatmap comparison of actual vs. predicted binary trait tables.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    sns.heatmap(actual_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[0])\n",
    "    axes[0].set_title(\"Actual Binary Table\\n\" + title)\n",
    "    sns.heatmap(predicted_binary, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False, ax=axes[1])\n",
    "    axes[1].set_title(\"Predicted Binary Table\\n\" + title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_comparison(actual_binary, predicted_binary, title):\n",
    "    \"\"\"Creates a grouped bar chart comparing the distribution (Yes counts) of actual vs. predicted values.\"\"\"\n",
    "    actual_counts = {col: actual_binary[col].sum() for col in actual_binary.columns}\n",
    "    predicted_counts = {col: predicted_binary[col].sum() for col in predicted_binary.columns}\n",
    "    dist_df = pd.DataFrame({\n",
    "        'Actual Yes': pd.Series(actual_counts),\n",
    "        'Predicted Yes': pd.Series(predicted_counts)\n",
    "    })\n",
    "    dist_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(\"Distribution Comparison\\n\" + title)\n",
    "    plt.xlabel(\"Trait Levels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Integrated Pipeline with Independent Test Set Using LOO\n",
    "#############################################\n",
    "\n",
    "def train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels, test_size=0.3, random_state=42):\n",
    "    results = {}\n",
    "    logging.info(f\"Processing trait: {target_trait}\")\n",
    "    \n",
    "    # Ensure Y_aligned is a pandas Series\n",
    "    if not isinstance(Y_aligned, pd.Series):\n",
    "        Y_aligned = pd.Series(Y_aligned, index=X_aligned.index)\n",
    "    \n",
    "    # Split the data into training and test sets (using stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_aligned, Y_aligned, test_size=test_size, random_state=random_state, stratify=Y_aligned\n",
    "    )\n",
    "    logging.info(f\"Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n",
    "    \n",
    "    # --- Build Binary Table for Test Set (for later comparison) ---\n",
    "    actual_binary_test = create_binary_trait_table(Y_test, target_trait, trait_levels)\n",
    "    \n",
    "    # --- Main ML Pipeline with Grid Search on Training Set ---\n",
    "    pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier(random_state=random_state))\n",
    "    ])\n",
    "    \n",
    "    param_grid = [\n",
    "        {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [RandomForestClassifier(random_state=random_state, class_weight='balanced')], # Add class balancing when using oxygen ass levels are unbalanced\n",
    "            'estimator__n_estimators': [100, 200],\n",
    "            'estimator__max_depth': [5, 10, None]\n",
    "        },\n",
    "        {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [SVC(random_state=random_state)],\n",
    "            'estimator__C': [0.1, 1, 10],\n",
    "            'estimator__kernel': ['linear', 'rbf'],\n",
    "            'estimator__gamma': ['scale', 'auto'],\n",
    "            'estimator__class_weight': [None, 'balanced']\n",
    "        },\n",
    "        {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [LogisticRegression(max_iter=1000, random_state=random_state)],\n",
    "            'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "        },\n",
    "        {\n",
    "            'select_k__k': [10, 50, 100, 200, 300, 500, 1000],\n",
    "            'estimator': [BernoulliNB()],\n",
    "            'estimator__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "            'estimator__binarize': [0.0]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Use Leave-One-Out CV on the training set for grid search\n",
    "    loo_train = LeaveOneOut()\n",
    "    logging.info(\"Starting grid search with Leave-One-Out CV on the training set.\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=loo_train, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    logging.info(f\"Best parameters for {target_trait}: {grid_search.best_params_}\")\n",
    "    logging.info(f\"Best CV score for {target_trait}: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # --- Evaluate the Final Model on the Independent Test Set ---\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    mcc_test = matthews_corrcoef(Y_test, Y_pred_test)\n",
    "    f1_test = f1_score(Y_test, Y_pred_test, average='macro')\n",
    "    logging.info(f\"Test Set Evaluation - MCC: {mcc_test:.3f}, F1 Score: {f1_test:.3f}\")\n",
    "    \n",
    "    # Plot test set evaluation results\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred_test)\n",
    "    class_labels = sorted(list(set(Y_test) | set(Y_pred_test)))  # Get all unique class labels\n",
    "    plot_confusion_matrix(cm_test, f\"{target_trait} Classifier (Test Set)\", class_labels)\n",
    "    \n",
    "    # Create binary table for predicted test labels and compare with actual\n",
    "    predicted_series_test = pd.Series(Y_pred_test, index=Y_test.index)\n",
    "    predicted_binary_test = create_binary_trait_table(predicted_series_test, target_trait, trait_levels)\n",
    "    \n",
    "    plot_side_by_side_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    plot_distribution_comparison(actual_binary_test, predicted_binary_test, f\"{target_trait} (Test Set)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "#######################\n",
    "# Trait selection\n",
    "#######################\n",
    "\n",
    "# Oxygen:\n",
    "target_trait = \"oxygen\"\n",
    "trait_levels = ['aerobic', 'aerotolerant', 'microaerophilic', 'obligate_aerobic', 'anaerobic', 'obligate_anaerobic', 'conflict', 'facultative']\n",
    "\n",
    "# Gramstain:\n",
    "#target_trait = \"gram\"\n",
    "#trait_levels = ['positive', 'negative']\n",
    "\n",
    "# Trophy:\n",
    "#target_trait = \"trophy\"\n",
    "#trait_levels = ['photo', 'chemo', 'litho', 'hetero', 'organo', 'auto']\n",
    "\n",
    "results = train_and_evaluate_with_test_set(X_aligned, Y_aligned, target_trait, trait_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCC Vs. F1 Plot\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Assume that X_train and Y_train have been defined in the previous cell from your independent test split.\n",
    "# For example:\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(...)\n",
    "\n",
    "# Define the range of k values to explore (number of features to select)\n",
    "k_values = range(1, 1000, 20)\n",
    "\n",
    "# Define the estimators to compare\n",
    "estimators = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'SupportVectorMachines': SVC(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'BernoulliNB': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Prepare a dictionary to store results (for both F1 and MCC scores)\n",
    "results = {name: {'f1': [], 'mcc': []} for name in estimators}\n",
    "\n",
    "# Initialize StratifiedKFold cross-validation with 5 splits on the training set\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Loop over each estimator and each k value\n",
    "for name, estimator in estimators.items():\n",
    "    print(f\"Processing estimator: {name}\")\n",
    "    for k in k_values:\n",
    "        print(f\"  Testing with k = {k}\")\n",
    "        # Create a pipeline that selects k features and applies the estimator\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif, k=k)),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "        # Evaluate F1-score using cross-validation on the training set\n",
    "        f1_scores = cross_val_score(\n",
    "            pipeline, X_train, Y_train, cv=cv,\n",
    "            scoring=make_scorer(f1_score, average='macro'),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        results[name]['f1'].append(f1_scores.mean())\n",
    "        \n",
    "        # Evaluate MCC using cross-validation on the training set\n",
    "        mcc_scores = cross_val_score(\n",
    "            pipeline, X_train, Y_train, cv=cv,\n",
    "            scoring=make_scorer(matthews_corrcoef),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        results[name]['mcc'].append(mcc_scores.mean())\n",
    "\n",
    "# For debugging: print out the results dictionary\n",
    "print(results)\n",
    "\n",
    "# Plot the results for each estimator for both F1 and MCC\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    k_values_list = list(k_values)  # Convert range to list for indexing\n",
    "    \n",
    "    # F1 Score Plot\n",
    "    finite_f1_scores = [score for score in scores['f1'] if np.isfinite(score)]\n",
    "    finite_k_values_f1 = [k for k, score in zip(k_values_list, scores['f1']) if np.isfinite(score)]\n",
    "    ax[0].plot(finite_k_values_f1, finite_f1_scores, marker='o', linestyle='-', label=name)\n",
    "    \n",
    "    # Annotate the highest F1 score for this estimator\n",
    "    if finite_f1_scores:\n",
    "        max_f1_score = max(finite_f1_scores)\n",
    "        max_f1_index = finite_f1_scores.index(max_f1_score)\n",
    "        best_k_f1 = finite_k_values_f1[max_f1_index]\n",
    "        ax[0].annotate(f'{max_f1_score:.2f}', (best_k_f1, max_f1_score),\n",
    "                       textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # MCC Score Plot\n",
    "    finite_mcc_scores = [score for score in scores['mcc'] if np.isfinite(score)]\n",
    "    finite_k_values_mcc = [k for k, score in zip(k_values_list, scores['mcc']) if np.isfinite(score)]\n",
    "    ax[1].plot(finite_k_values_mcc, finite_mcc_scores, marker='o', linestyle='-', label=name)\n",
    "    \n",
    "    # Annotate the highest MCC score for this estimator\n",
    "    if finite_mcc_scores:\n",
    "        max_mcc_score = max(finite_mcc_scores)\n",
    "        max_mcc_index = finite_mcc_scores.index(max_mcc_score)\n",
    "        best_k_mcc = finite_k_values_mcc[max_mcc_index]\n",
    "        ax[1].annotate(f'{max_mcc_score:.2f}', (best_k_mcc, max_mcc_score),\n",
    "                       textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Customize the F1 plot\n",
    "ax[0].set_title('F1 Score by Number of Selected Features (k) for Different Estimators (Training Set)')\n",
    "ax[0].set_xlabel('Number of Features (k)')\n",
    "ax[0].set_ylabel('F1 Score')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Customize the MCC plot\n",
    "ax[1].set_title('MCC Score by Number of Selected Features (k) for Different Estimators (Training Set)')\n",
    "ax[1].set_xlabel('Number of Features (k)')\n",
    "ax[1].set_ylabel('MCC Score')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEGG Pathway Mapping\n",
    "def map_ko_to_pathways(ko_terms):\n",
    "    kegg = KEGG()\n",
    "    pathways = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            gene_links = kegg.link(\"pathway\", ko)\n",
    "            if gene_links:\n",
    "                for entry in gene_links.strip().split(\"\\n\"):\n",
    "                    split_entry = entry.split(\"\\t\")\n",
    "                    if len(split_entry) >= 2:\n",
    "                        ko_id, pathway_id = split_entry[0], split_entry[1]\n",
    "                        if pathway_id not in pathways:\n",
    "                            pathways[pathway_id] = set()\n",
    "                        pathways[pathway_id].add(ko)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ko}: {e}\")\n",
    "    return pathways\n",
    "\n",
    "selected_important_features = X_terms.columns[selector.get_support()]\n",
    "pathways = map_ko_to_pathways(selected_important_features)\n",
    "\n",
    "# Creating the adjacency matrix with translated KO terms, including original KO term\n",
    "translated_kos = {ko: f\"Translated_{ko}\" for ko in selected_important_features}  # Placeholder for actual translation function\n",
    "pathway_matrix = pd.DataFrame(\n",
    "    index=[f\"{translated_kos[ko]} ({ko})\" for ko in selected_important_features],\n",
    "    columns=pathways.keys(),\n",
    "    data=0\n",
    ")\n",
    "for pathway, kos in pathways.items():\n",
    "    for ko in kos:\n",
    "        if ko in selected_important_features:\n",
    "            pathway_matrix.loc[f\"{translated_kos[ko]} ({ko})\", pathway] = 1\n",
    "\n",
    "# Fetch and rename pathway names for readability\n",
    "kegg = KEGG()\n",
    "for column in pathway_matrix.columns:\n",
    "    pathway_info = kegg.get(column)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else column\n",
    "    pathway_matrix.rename(columns={column: pathway_name}, inplace=True)\n",
    "\n",
    "print(\"Pathway matrix after renaming:\\n\", pathway_matrix)\n",
    "\n",
    "# Heatmap visualization\n",
    "sns.heatmap(pathway_matrix, annot=True, cmap=\"Greys\", cbar=False)\n",
    "plt.title(f'Adjacency Matrix of KO Terms and Pathways ({trait_column})')\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('KO Terms')\n",
    "plt.show()\n",
    "\n",
    "# Network Visualization\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define a list of general pathways to exclude\n",
    "excluded_pathways = [\"metabolic pathways\"]  # You can add more general terms here\n",
    "\n",
    "# Add nodes and edges with renamed pathway names\n",
    "for ko in selected_important_features:\n",
    "    translated_label = f\"{translated_kos[ko]} ({ko})\"\n",
    "    G.add_node(ko, title=translated_label, label=translated_label, color='red', size=20)\n",
    "\n",
    "for pathway_id, kos in pathways.items():\n",
    "    pathway_info = kegg.get(pathway_id)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else pathway_id\n",
    "    if pathway_name.lower() not in excluded_pathways:\n",
    "        G.add_node(pathway_name, title=pathway_name, label=pathway_name, color='blue', size=30)\n",
    "        for ko in kos:\n",
    "            G.add_edge(ko, pathway_name)\n",
    "\n",
    "# Pyvis network visualization\n",
    "nt = Network(\"800px\", \"1200px\", notebook=True, heading=f'Interactive Network of KO Terms and Pathways ({trait_column})', bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources='remote')\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "nt.save_graph(f\"ko_network_{trait_column}.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
