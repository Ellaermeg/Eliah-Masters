{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\eliah\\\\Documents\\\\Master\\\\Eliah-Masters\\\\KOs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\\\python312.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0', '', 'C:\\\\Users\\\\eliah\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\eliah\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\eliah\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\eliah\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\Pythonwin', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\\\Lib\\\\site-packages', '../Data_Feature', '../Datasets', '../Data_Feature', '../Datasets', '../Data_Feature', '../Datasets', '../Data_Feature', '../Datasets', '../Data_Feature', '../Datasets']\n",
      "Features with zero variance after filtering: 0\n",
      "        oxygen\n",
      "key           \n",
      "1      aerobic\n",
      "2      aerobic\n",
      "3      aerobic\n",
      "4    anaerobic\n",
      "5      aerobic\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Datasets')\n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB # or GaussianNB if your data is normalized and continuous\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, make_scorer, matthews_corrcoef\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import zipfile \n",
    "from bioservices import KEGG\n",
    "from K_func import translate_ko_terms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Data prep and feature mapping\n",
    "'''\n",
    "\n",
    "os.chdir('../Datasets')\n",
    "# Paths to your uploaded files\n",
    "zip_file_path_KO = \"terms_KO.zip\"\n",
    "csv_file_name_KO = 'terms_KO.csv'\n",
    "zip_file_path_traits = 'reducedDataset.zip'\n",
    "csv_file_name_traits = 'reducedDataset.csv'\n",
    "# Read and preprocess KO terms\n",
    "with zipfile.ZipFile(zip_file_path_KO, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_KO) as file:\n",
    "        KOs = pd.read_csv(file, index_col=0)\n",
    "\n",
    "KOs['value'] = 1\n",
    "X_terms = KOs.pivot_table(index='key', columns='KO', values='value', fill_value=0)\n",
    "\n",
    "# Apply VarianceThreshold to remove constant features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_filtered = selector.fit_transform(X_terms)\n",
    "X_filtered_df = pd.DataFrame(X_filtered, index=X_terms.index, columns=X_terms.columns[selector.get_support()])\n",
    "\n",
    "# Optionally, inspect the variance of features to confirm removal\n",
    "feature_variances = X_filtered_df.var()\n",
    "print(f\"Features with zero variance after filtering: {sum(feature_variances == 0)}\")\n",
    "\n",
    "# Read and preprocess traits data\n",
    "with zipfile.ZipFile(zip_file_path_traits, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_traits) as file:\n",
    "        traits = pd.read_csv(file, sep=';')\n",
    "traits['oxygen'] = traits['oxygen'].str.lower()\n",
    "\n",
    "# Mapping of oxygen levels to a simplified classification\n",
    "oxygen_mapping = {\n",
    "    'aerobic': 'aerobic',\n",
    "    'aerotolerant': 'aerobic',\n",
    "    'microaerophilic': 'aerobic',\n",
    "    'obligate aerobic': 'aerobic',\n",
    "    'anaerobic': 'anaerobic',\n",
    "    'obligate anaerobic': 'anaerobic',\n",
    "    'conflict': 'aerobic',  \n",
    "    'facultative': 'aerobic'  \n",
    "}\n",
    "traits['oxygen'] = traits['oxygen'].map(oxygen_mapping)\n",
    "\n",
    "\n",
    "# Handling missing values and aggregating by key\n",
    "y = traits.dropna(subset=['oxygen']).groupby('key').agg({'oxygen': lambda x: x.value_counts().index[0]})\n",
    "print(y.head())\n",
    "\n",
    "# Find common keys after removing missing values\n",
    "common_keys = X_filtered_df.index.intersection(y.index)\n",
    "\n",
    "# Align X (features) and Y (labels) based on common keys\n",
    "X_aligned = X_filtered_df.loc[common_keys]\n",
    "Y_aligned = y.loc[common_keys].values.ravel()\n",
    "\n",
    "# Ensures X_aligned and Y_aligned are aligned\n",
    "assert X_aligned.shape[0] == len(Y_aligned), \"X and Y are not aligned\"\n",
    "\n",
    "# waklka da imnar er ger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO    K00001  K00002  K00003  K00004  K00005  K00007  K00008  K00009  K00010  \\\n",
      "key                                                                            \n",
      "1        0.0     0.0     1.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
      "2        1.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3        1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4        1.0     0.0     1.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
      "5        0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "3303     0.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3304     1.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n",
      "3305     0.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3306     1.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3307     0.0     0.0     1.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "KO    K00011  ...  K22477  K22478  K22479  K22486  K22489  K22491  K22502  \\\n",
      "key           ...                                                           \n",
      "1        0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2        0.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3        0.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "4        0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "5        0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "3303     0.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3304     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3305     0.0  ...     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "3306     0.0  ...     0.0     0.0     0.0     0.0     1.0     1.0     0.0   \n",
      "3307     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "KO    K22504  K22506  K22510  \n",
      "key                           \n",
      "1        0.0     0.0     0.0  \n",
      "2        0.0     0.0     0.0  \n",
      "3        0.0     0.0     0.0  \n",
      "4        0.0     0.0     0.0  \n",
      "5        0.0     0.0     0.0  \n",
      "...      ...     ...     ...  \n",
      "3303     0.0     0.0     0.0  \n",
      "3304     0.0     0.0     0.0  \n",
      "3305     0.0     0.0     0.0  \n",
      "3306     0.0     0.0     0.0  \n",
      "3307     0.0     0.0     0.0  \n",
      "\n",
      "[3256 rows x 7149 columns] ['aerobic' 'aerobic' 'aerobic' ... 'aerobic' 'aerobic' 'aerobic']\n"
     ]
    }
   ],
   "source": [
    "print(X_aligned, Y_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculating Pearsons coefficient'''\n",
    "\n",
    "# Always look at data, right?\n",
    "print(X_aligned.head())\n",
    "\n",
    "\n",
    "'''Handelig data values'''\n",
    "\n",
    "# Checking datatype for y_algned, i think\n",
    "class_distribution = pd.Series(Y_aligned).value_counts()\n",
    "print(\"Class distribution in Y_aligned:\", class_distribution)\n",
    "\n",
    "# Checks data label of Y algined\n",
    "unique_labels = np.unique(Y_aligned)\n",
    "print(f\"Unique labels in Y_aligned: {unique_labels}\")\n",
    "\n",
    "# Initialize the LabelEncoder and transform the data from catagorical\n",
    "label_encoder = LabelEncoder()\n",
    "Y_aligned = label_encoder.fit_transform(Y_aligned)\n",
    "\n",
    "# Check data once labelencoder is done\n",
    "print(f\"Data of Y_aligned: {np.unique(Y_aligned)}\")\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "corr_coefficients = [] # Collectes coefficents from X_aligned and Y_aligned calculations\n",
    "\n",
    "# I should probably check how pearsons actually works\n",
    "\n",
    "\n",
    "for column in X_aligned.columns: # Iterates over every column in X_aligned\n",
    "    # Easier to check all the different collumns because its a pandas data frame\n",
    "    # Interger indexing is cool but i dont like it\n",
    "    r, p = pearsonr(X_aligned[column], Y_aligned)  \n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "\n",
    "# PLotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=100, color='red', edgecolor='black')\n",
    "plt.title('Correlations')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pipeline implementation for Naive bayes and random forests'''\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_aligned, Y_aligned, test_size=0.3, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('select_k', SelectKBest(f_classif)),\n",
    "    ('estimator', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [RandomForestClassifier(random_state=42)],\n",
    "        'estimator__n_estimators': [100, 200],  \n",
    "        'estimator__max_depth': [5, 10, None]  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  # May need as needed  100, 200, 300\n",
    "        'estimator': [SVC(random_state=42)],\n",
    "        'estimator__C': [0.1, 1, 10],  \n",
    "        'estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__gamma': ['scale', 'auto']  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k' : [10, 50, 100, 200, 300, 500, 1000],\n",
    "        'estimator': [LogisticRegression()],\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },         \n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [BernoulliNB()],\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0, 10.0],  \n",
    "        'estimator__binarize': [0.0]  # Is automatically applied\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "''' Grid search '''\n",
    "\n",
    "# Set up GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "'''MCC shenanigans'''\n",
    "\n",
    "mcc = matthews_corrcoef(Y_test, Y_pred)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc}\")\n",
    "\n",
    "\n",
    "# Display confusion matrix for the best model\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title(f'Confusion Matrix for the Best Model')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Finding best traits '''\n",
    "\n",
    "'''# Finding most predictive features with respect to target value (oxygen)\n",
    "select_k_best = SelectKBest(f_classif, k=10)  \n",
    "X_new = select_k_best.fit_transform(X_aligned, Y_aligned)\n",
    "selected_features = select_k_best.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected KO terms\n",
    "selected_ko_terms = X_aligned.columns[selected_features]\n",
    "\n",
    "# Fitting select K best to find the scores\n",
    "select_k_best.fit(X_aligned, Y_aligned)\n",
    "scores = select_k_best.scores_[selected_features] # scores of selected features\n",
    "\n",
    "# Sorting features by their scores\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "sorted_scores = scores[sorted_indices]\n",
    "sorted_features = selected_ko_terms[sorted_indices]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner (F score):\")\n",
    "print(sorted_features)\n",
    "\n",
    "#Plot for visualizing distribution and importance of best traits\n",
    "\n",
    "# Translate selected KO terms to their descriptions\n",
    "selected_ko_terms_list = list(sorted_features)  \n",
    "ko_descriptions_mapping = translate_ko_terms(selected_ko_terms_list)\n",
    "\n",
    "\n",
    "# Replace KO terms with their descriptions for plotting\n",
    "translated_sorted_features = [ko_descriptions_mapping[ko] for ko in sorted_features]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(translated_sorted_features)), sorted_scores)\n",
    "plt.xticks(range(len(translated_sorted_features)), translated_sorted_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('F-scores')\n",
    "plt.title('Top 50 KO Descriptions by F-score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model based feature importance score '''\n",
    "\n",
    "#Fit a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their importances\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices = sorted_indices[:10]\n",
    "selected_features = X_aligned.columns[top_k_indices]\n",
    "\n",
    "# Scores of selected features\n",
    "sorted_scores = feature_importances[top_k_indices]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner based on randomforest classifier:\")\n",
    "print(\"This is model based feature importance (uses models own feature importance score to evaluate most important ones)\")\n",
    "print(selected_features)\n",
    "\n",
    "#Translate selected KO terms to their descriptions\n",
    "translated_sorted_features = translate_ko_terms(list(selected_features))\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels = [translated_sorted_features[ko] for ko in selected_features]\n",
    "sorted_labels = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels)), sorted_scores)\n",
    "plt.xticks(range(len(sorted_labels)), selected_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('Importance Scores')\n",
    "plt.title('Top 10 KO Descriptions by Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a LogisticRegression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their coefficients\n",
    "lr_coefficients = lr_model.coef_[0]\n",
    "sorted_indices = np.argsort(np.abs(lr_coefficients))[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices = sorted_indices[:10]\n",
    "selected_features = X_aligned.columns[top_k_indices]\n",
    "\n",
    "# Coefficients (importance) of selected features\n",
    "sorted_scores = lr_coefficients[top_k_indices]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner based on logistic regression:\")\n",
    "print(\"This is model based feature importance (uses model's coefficients as importance scores)\")\n",
    "print(selected_features)\n",
    "\n",
    "#Translate selected KO terms to their descriptions\n",
    "translated_sorted_features = translate_ko_terms(list(selected_features))\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels = [translated_sorted_features[ko] for ko in selected_features]\n",
    "sorted_labels = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels)), np.abs(sorted_scores))  # Use absolute value for visualization\n",
    "plt.xticks(range(len(sorted_labels)), translated_sorted_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('Coefficient Magnitudes')\n",
    "plt.title('Top 10 KO Descriptions by Logistic Regression Coefficient Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' TROUBLE SHOOTING '''\n",
    "\n",
    "# Check the distribution of classes in Y_train\n",
    "class_distribution = pd.Series(Y_train).value_counts()\n",
    "print(\"Class distribution in Y_train:\\n\", class_distribution)\n",
    "\n",
    "# Check unique labels in Y_train\n",
    "unique_labels = np.unique(Y_train)\n",
    "print(f\"Unique labels in Y_train: {unique_labels}\")\n",
    "\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform Y_train to encode labels\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "# Replace Y_train with the encoded labels if necessary\n",
    "Y_train = Y_train_encoded\n",
    "\n",
    "# Now, you can inspect the unique values of the encoded Y_train\n",
    "print(f\"Unique encoded labels in Y_train: {np.unique(Y_train)}\")\n",
    "\n",
    "\n",
    "# Check if the number of instances matches\n",
    "print(f\"Number of instances in X_train: {X_train.shape[0]}\")\n",
    "print(f\"Number of labels in Y_train: {len(Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Comparing F1 and MCC plots'''\n",
    "\n",
    "# Define the range of `k` values to explore\n",
    "k_values = range(1, 1000, 20)  # Range of steps\n",
    "\n",
    "# Define estimators to compare\n",
    "estimators = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'SupportVectorMachines': SVC(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'BernoulliNB': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Prepare to store results for both F1 and MCC\n",
    "results = {name: {'f1': [], 'mcc': []} for name in estimators}\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Loop over each estimator\n",
    "for name, estimator in estimators.items():\n",
    "    print(f\"Processing estimator: {name}\")  # Debug print\n",
    "    # Loop over each `k` value\n",
    "    for k in k_values:\n",
    "        print(f\"Testing with k={k}\")  # Debug print\n",
    "        # Define the pipeline for the current estimator\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif, k=k)),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation for F1-score\n",
    "        f1_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "        results[name]['f1'].append(f1_scores.mean())\n",
    "        \n",
    "        # Perform cross-validation for MCC\n",
    "        mcc_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(matthews_corrcoef), n_jobs=-1)\n",
    "        results[name]['mcc'].append(mcc_scores.mean())\n",
    "\n",
    "# For debugging \n",
    "print(results)\n",
    "\n",
    "''' Plotting '''\n",
    "\n",
    "# Plotting the results for each estimator for both F1 and MCC\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    k_values_list = list(k_values)  # Convert range to list for indexing\n",
    "    # F1 Plot\n",
    "    finite_f1_scores = [score for score in scores['f1'] if np.isfinite(score)]\n",
    "    finite_k_values_f1 = [k for k, score in zip(k_values_list, scores['f1']) if np.isfinite(score)]\n",
    "    ax[0].plot(finite_k_values_f1, finite_f1_scores, marker='o', linestyle='-', label=name)\n",
    "    \n",
    "    # highest F1 score\n",
    "    if finite_f1_scores:  # Check if there are any finite scores\n",
    "        max_f1_score = max(finite_f1_scores)\n",
    "        max_f1_index = finite_f1_scores.index(max_f1_score)\n",
    "        best_k_f1 = finite_k_values_f1[max_f1_index]\n",
    "        ax[0].annotate(f'{max_f1_score:.2f}', (best_k_f1, max_f1_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # MCC Plot\n",
    "    finite_mcc_scores = [score for score in scores['mcc'] if np.isfinite(score)]\n",
    "    finite_k_values_mcc = [k for k, score in zip(k_values_list, scores['mcc']) if np.isfinite(score)]\n",
    "    ax[1].plot(finite_k_values_mcc, finite_mcc_scores, marker='o', linestyle='-', label=name)\n",
    "\n",
    "    #  highest MCC score\n",
    "    if finite_mcc_scores:  # Check if there are any finite scores\n",
    "        max_mcc_score = max(finite_mcc_scores)\n",
    "        max_mcc_index = finite_mcc_scores.index(max_mcc_score)\n",
    "        best_k_mcc = finite_k_values_mcc[max_mcc_index]\n",
    "        ax[1].annotate(f'{max_mcc_score:.2f}', (best_k_mcc, max_mcc_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    \n",
    "\n",
    "ax[0].set_title('F1 Score by Number of Selected Features (k) for Different Estimators')\n",
    "ax[0].set_xlabel('Number of Features (k)')\n",
    "ax[0].set_ylabel('F1 Score')\n",
    "\n",
    "ax[1].set_title('MCC by Number of Selected Features (k) for Different Estimators')\n",
    "ax[1].set_xlabel('Number of Features (k)')\n",
    "ax[1].set_ylabel('MCC Score')\n",
    "\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot for LR vs RFS\n",
    "\n",
    "# Fit models\n",
    "lr_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting to aligned matrixes\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "# Get coefficients and feature importances\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "sorted_indices_rf = np.argsort(rf_feature_importance)[::-1]\n",
    "sorted_indices_lr = np.argsort(logistic_importance)[::-1]\n",
    "\n",
    "\n",
    "top_k_indices_rf = sorted_indices_rf[:10]\n",
    "top_k_indices_lr = sorted_indices_lr[:10]\n",
    "selected_features_rf = X_aligned.columns[top_k_indices_rf]\n",
    "selected_features_lr = X_aligned.columns[top_k_indices_lr]\n",
    "\n",
    "print(selected_features_rf)\n",
    "print(selected_features_lr)\n",
    "\n",
    "sorted_scores_rf = rf_feature_importance[top_k_indices_rf]\n",
    "sorted_scores_lr = logistic_importance[top_k_indices_lr]\n",
    "\n",
    "\n",
    "# Normalize the importance scores\n",
    "scaler = MinMaxScaler()\n",
    "lr_importance_scaled = scaler.fit_transform(logistic_importance.reshape(-1, 1)).flatten()\n",
    "rf_importance_scaled = scaler.fit_transform(rf_feature_importance.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(lr_importance_scaled, rf_importance_scaled, alpha=0.6, color='red')\n",
    "\n",
    "# Line of best fit\n",
    "m, b = np.polyfit(rf_importance_scaled, lr_importance_scaled, 1)\n",
    "ax.plot(rf_importance_scaled, m*rf_importance_scaled + b, color='blue')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Logistic Regression Importance Scores (LR)')\n",
    "ax.set_ylabel('Random Forest Importance Scores (RFS)')\n",
    "ax.set_title('Comparison of Feature Importance Scores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Important to remember\n",
    "\n",
    "''' \n",
    "\n",
    "#Logistic Regression Coefficients: After normalizing, they represent the relative predictive strength of features, \n",
    "#but they lose their direct probabilistic interpretation.\n",
    "\n",
    "#Random Forest Feature Importances: They indicate how much each feature contributes to the predictive accuracy of the model, \n",
    "#based on how much each feature decreases the impurity of the splits.\n",
    "\n",
    "#Maybe test rankings of features instead of normalized values. Can compare and see which features are consistently considered\n",
    "#important across both models\n",
    "'''\n",
    "\n",
    "\n",
    "# Rank the features by importance for each model\n",
    "lr_ranking = np.argsort(logistic_importance)[::-1]  # argsort returns indices that would sort the array\n",
    "rf_ranking = np.argsort(rf_feature_importance)[::-1]\n",
    "\n",
    "\n",
    "# For visualization\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = X_aligned.columns  # For feature names\n",
    "ranking_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'LR_Rank': lr_ranking.argsort(),\n",
    "    'RF_Rank': rf_ranking.argsort(),\n",
    "    'LR_Score': logistic_importance[lr_ranking.argsort()],\n",
    "    'RF_Score': rf_feature_importance[rf_ranking.argsort()]\n",
    "})\n",
    "\n",
    "\n",
    "# Select top features by rank threshold or specific indices if needed\n",
    "top_n = 10\n",
    "ranking_df['Top_LR'] = ranking_df['LR_Rank'] < top_n\n",
    "ranking_df['Top_RF'] = ranking_df['RF_Rank'] < top_n\n",
    "\n",
    "# Now print or use this DataFrame for visualization\n",
    "print(ranking_df.head(20))  # Display the first 20 entries for checking\n",
    "\n",
    "''' \n",
    "# Sorts based on logisitc regression rank\n",
    "#ranking_df.sort_values('LR_Rank', inplace=True)\n",
    "# Can also sort by 'RF_Rank' \n",
    "# Ranks from random forests\n",
    "#print(ranking_df[['Feature', 'LR_Rank', 'RF_Rank']].head(100))\n",
    "# This shows the top 20 features as ranked by logistic regression, alongside their ranks in random forest\n",
    "'''\n",
    "\n",
    "# To find features that are consistently considered important, look for low rank numbers in both models\n",
    "consistently_important_features = ranking_df[(ranking_df['LR_Rank'] < 100) & (ranking_df['RF_Rank'] < 100)]\n",
    "print(consistently_important_features)\n",
    "\n",
    "\n",
    "\n",
    "\"importance\" in logistic regression is based on the magnitude of the coefficients (which reflect the odds of the outcome), \n",
    "#while in random forests, importance is derived from how much a feature decreases the impurity of the splits.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_aligned and Y_aligned are defined and appropriate for model fitting\n",
    "\n",
    "# Fit models\n",
    "lr_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting to aligned matrices\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "# Get coefficients and feature importances\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Rank the features by importance for each model\n",
    "lr_ranking = np.argsort(logistic_importance)[::-1]  # argsort returns indices that would sort the array\n",
    "rf_ranking = np.argsort(rf_feature_importance)[::-1]\n",
    "\n",
    "# Normalize the importance scores\n",
    "scaler = MinMaxScaler()\n",
    "lr_importance_scaled = scaler.fit_transform(logistic_importance.reshape(-1, 1)).flatten()\n",
    "rf_importance_scaled = scaler.fit_transform(rf_feature_importance.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(lr_importance_scaled, rf_importance_scaled, alpha=0.6, color='red')\n",
    "\n",
    "# Line of best fit\n",
    "m, b = np.polyfit(rf_importance_scaled, lr_importance_scaled, 1)\n",
    "ax.plot(rf_importance_scaled, m*rf_importance_scaled + b, color='blue')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Logistic Regression Importance Scores (LR)')\n",
    "ax.set_ylabel('Random Forest Importance Scores (RFS)')\n",
    "ax.set_title('Comparison of Feature Importance Scores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For visualization\n",
    "feature_names = X_aligned.columns  # Assuming your DataFrame columns are the feature names\n",
    "ranking_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'LR_Rank': lr_ranking.argsort(),  # Convert indices to rank positions\n",
    "    'RF_Rank': rf_ranking.argsort(),\n",
    "    'LR_Score': logistic_importance[lr_ranking.argsort()],  # Order scores by original feature order\n",
    "    'RF_Score': rf_feature_importance[rf_ranking.argsort()]\n",
    "})\n",
    "\n",
    "# Sort DataFrame based on what is needed\n",
    "ranking_df.sort_values('RF_Rank', ascending=True, inplace=True)\n",
    "\n",
    "# Select top features by rank threshold or specific indices if needed\n",
    "top_n = 10\n",
    "ranking_df['Top_LR'] = ranking_df['LR_Rank'] < top_n\n",
    "ranking_df['Top_RF'] = ranking_df['RF_Rank'] < top_n\n",
    "\n",
    "# Display the first 20 entries for checking\n",
    "print(ranking_df.head(20))\n",
    "\n",
    "# To find features that are consistently considered important, look for low rank numbers in both models\n",
    "consistently_important_features = ranking_df[(ranking_df['LR_Rank'] < 100) & (ranking_df['RF_Rank'] < 100)]\n",
    "print(consistently_important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pearson R for correlation coefficient between feature importance scores (From both models)'''\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(solver='liblinear', penalty=\"l1\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_split=3, random_state=42)\n",
    "\n",
    "\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr, p_value = pearsonr(logistic_importance, rf_feature_importance)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr:.3f}\")\n",
    "print(f\"P-Value: {p_value:.3f}\")\n",
    "\n",
    "# Bootstrap Pearson correlation coefficients\n",
    "n_bootstraps = 10000\n",
    "corr_coefficients = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Sample indices with replacement\n",
    "    indices = resample(np.arange(len(logistic_importance)))\n",
    "    \n",
    "    # Calculate Pearson correlation for the sample\n",
    "    r, _ = pearsonr(logistic_importance[indices], rf_feature_importance[indices])\n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=30, color='red', edgecolor='black')\n",
    "plt.title('Bootstrap Dist. of Pearson Correlation Coefficients')\n",
    "plt.xlabel('Pearson Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments on the plot:\n",
    "\n",
    "Bootstrapping was done to simulate calculating pearsons with multiple variables against a single response variable. The bootstrappiing segments the data to generate a distribution of pearson coeffiecients\n",
    "\n",
    "Bootstrapping randomly samples the data with replacemnet, calculating pearsons for each sample and reapting this many times for the distribution\n",
    "\n",
    "Gives a sense os how the relationship between LR and RF mighjt vary across different random samples of the data\n",
    "\n",
    "Pvalue is 0.00???\n",
    "\n",
    "The weak but positive relationship suggests that when features are considered important in one model, this also seems to be the case in the other.\n",
    "However, degree of importance does not seem to be aligning\n",
    "\n",
    "Positive but not strong relationship.\n",
    "\n",
    "Logisitic regression may be more sensitive to linear relationships while randomforestes may capture none-linear dependancies better.\n",
    "\n",
    "Pearson correlation coefficent is invariant, but the way the feature impact the model output probably varies a lot. (If feature have different scales, which i think they do, at least the importance)\n",
    "\n",
    "LR gives negative or positive coefficents, refelcting the direction and strength of the relationship with the dependant variable\n",
    "RF uses a measure based on how much each feature decreases the impurity of the splits. This wont nesecarilly imply direction, only the usefullness or utility of the feature in node splitting\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
