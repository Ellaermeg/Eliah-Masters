{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Elijah\\\\Documents\\\\THESIS\\\\Eliah-Masters\\\\KOs', 'C:\\\\Program Files\\\\PerkinElmerInformatics\\\\ChemOffice2020\\\\ChemScript\\\\Lib', 'c:\\\\Python312\\\\python312.zip', 'c:\\\\Python312\\\\DLLs', 'c:\\\\Python312\\\\Lib', 'c:\\\\Python312', '', 'C:\\\\Users\\\\Elijah\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\Elijah\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Elijah\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Elijah\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\Pythonwin', 'c:\\\\Python312\\\\Lib\\\\site-packages', '../Data_Feature', '../Datasets']\n",
      "Features with zero variance after filtering: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "X and Y are not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 88\u001b[0m\n\u001b[0;32m     84\u001b[0m Y_aligned \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mloc[common_keys]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Ensures X_aligned and Y_aligned are aligned\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_aligned\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_aligned), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX and Y are not aligned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_aligned\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_aligned\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAssertionError\u001b[0m: X and Y are not aligned"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Datasets')\n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, make_scorer, matthews_corrcoef, roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import plotly.figure_factory as ff\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import zipfile \n",
    "from bioservices import KEGG\n",
    "from K_func import translate_ko_terms\n",
    "\n",
    "\n",
    "'''\n",
    "Data prep and feature mapping\n",
    "'''\n",
    "\n",
    "os.chdir('../Datasets')\n",
    "# Paths to your uploaded files\n",
    "zip_file_path_KO = \"terms_KO.zip\"\n",
    "csv_file_name_KO = 'terms_KO.csv'\n",
    "zip_file_path_traits = 'assembledDataset.zip'\n",
    "csv_file_name_traits = 'assembledDataset.csv'\n",
    "# Read and preprocess KO terms\n",
    "with zipfile.ZipFile(zip_file_path_KO, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_KO) as file:\n",
    "        KOs = pd.read_csv(file, index_col=0)\n",
    "\n",
    "KOs['value'] = 1\n",
    "X_terms = KOs.pivot_table(index='key', columns='KO', values='value', fill_value=0)\n",
    "\n",
    "# Apply VarianceThreshold to remove constant features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_filtered = selector.fit_transform(X_terms)\n",
    "X_filtered_df = pd.DataFrame(X_filtered, index=X_terms.index, columns=X_terms.columns[selector.get_support()])\n",
    "\n",
    "# Optionally, inspect the variance of features to confirm removal\n",
    "feature_variances = X_filtered_df.var()\n",
    "print(f\"Features with zero variance after filtering: {sum(feature_variances == 0)}\")\n",
    "\n",
    "# Read and preprocess traits data\n",
    "with zipfile.ZipFile(zip_file_path_traits, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_traits) as file:\n",
    "        traits = pd.read_csv(file, low_memory=False, sep=';')\n",
    "\n",
    "# Handle missing values and prepare data for trophic levels\n",
    "traits['trophy'] = traits['trophy'].str.lower()  # assuming trophic levels are stored as strings\n",
    "traits = traits.dropna(subset=['trophy'])\n",
    "\n",
    "# Define binary labels for each trophic level\n",
    "trophic_levels = ['photo', 'chemo', 'mixed', 'litho', 'hetero', 'organo']\n",
    "\n",
    "# Initialize a dictionary to store data for each trophic level\n",
    "binary_labels = {}\n",
    "\n",
    "for trophy in trophic_levels:\n",
    "    binary_labels[trophy] = traits['trophy'].apply(lambda x: 1 if trophy in x else 0)\n",
    "\n",
    "# Combine binary labels into a DataFrame\n",
    "y = pd.DataFrame(binary_labels)\n",
    "\n",
    "# Align X (features) and Y (labels) based on common keys\n",
    "common_keys = X_filtered_df.index.intersection(y.index)\n",
    "\n",
    "\n",
    "X_aligned = X_filtered_df.loc[common_keys]\n",
    "Y_aligned = y.loc[common_keys].values.ravel()\n",
    "\n",
    "\n",
    "# Ensures X_aligned and Y_aligned are aligned\n",
    "assert X_aligned.shape[0] == len(Y_aligned), \"X and Y are not aligned\"\n",
    "\n",
    "print(X_aligned.shape)\n",
    "print(Y_aligned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculating Pearsons coefficient'''\n",
    "\n",
    "# Always look at data, right?\n",
    "print(X_aligned.head())\n",
    "\n",
    "\n",
    "'''Handelig data values'''\n",
    "\n",
    "# Checking datatype for y_algned, i think\n",
    "class_distribution = pd.Series(Y_aligned).value_counts()\n",
    "print(\"Class distribution in Y_aligned:\", class_distribution)\n",
    "\n",
    "# Checks data label of Y algined\n",
    "unique_labels = np.unique(Y_aligned)\n",
    "print(f\"Unique labels in Y_aligned: {unique_labels}\")\n",
    "\n",
    "# Initialize the LabelEncoder and transform the data from catagorical\n",
    "label_encoder = LabelEncoder()\n",
    "Y_aligned = label_encoder.fit_transform(Y_aligned)\n",
    "\n",
    "# Check data once labelencoder is done\n",
    "print(f\"Data of Y_aligned: {np.unique(Y_aligned)}\")\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "corr_coefficients = [] # Collectes coefficents from X_aligned and Y_aligned calculations\n",
    "\n",
    "# I should probably check how pearsons actually works\n",
    "\n",
    "\n",
    "for column in X_aligned.columns: # Iterates over every column in X_aligned\n",
    "    # Easier to check all the different collumns because its a pandas data frame\n",
    "    # Interger indexing is cool but i dont like it\n",
    "    r, p = pearsonr(X_aligned[column], Y_aligned)  \n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "\n",
    "# PLotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=100, color='red', edgecolor='black')\n",
    "plt.title('Correlations')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pipeline implementation for Naive bayes and random forests'''\n",
    "\n",
    "results = {}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('select_k', SelectKBest(f_classif)),\n",
    "        ('estimator', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    # Define a parameter grid to search over\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [RandomForestClassifier(random_state=42)],\n",
    "        'estimator__n_estimators': [100, 200],  \n",
    "        'estimator__max_depth': [5, 10, None]  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  # May need as needed  100, 200, 300\n",
    "        'estimator': [SVC(random_state=42)],\n",
    "        'estimator__C': [0.1, 1, 10],  \n",
    "        'estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__gamma': ['scale', 'auto']  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k' : [10, 50, 100, 200, 300, 500, 1000],\n",
    "        'estimator': [LogisticRegression()],\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },         \n",
    "    {\n",
    "        'select_k__k': [10, 50, 100, 200, 300, 500, 1000],  \n",
    "        'estimator': [BernoulliNB()],\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0, 10.0],  \n",
    "        'estimator__binarize': [0.0]  # Is automatically applied\n",
    "    }\n",
    "]\n",
    "\n",
    "for trophy in trophic_levels:\n",
    "    print(f\"Training classifier for: {trophy}\")\n",
    "    \n",
    "    #Y_trophic = Y_aligned[trophy]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_aligned, Y_aligned, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "    ''' Grid search '''\n",
    "\n",
    "    # Set up GridSearchCV to find the best parameters\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters found:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    Y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "    '''MCC shenanigans'''\n",
    "\n",
    "    mcc = matthews_corrcoef(Y_test, Y_pred)\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc}\")\n",
    "\n",
    "    # Store results\n",
    "    results[trophy] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'mcc': mcc,\n",
    "        'confusion_matrix': confusion_matrix(Y_test, Y_pred)\n",
    "    }\n",
    "\n",
    "    # Display confusion matrix for the best model\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'Confusion Matrix for the Best Model')\n",
    "    plt.show()\n",
    "\n",
    "# Optional: Print summary of results\n",
    "for trophy, result in results.items():\n",
    "    print(f\"\\nSummary for {trophy}:\")\n",
    "    print(f\"Best Params: {result['best_params']}\")\n",
    "    print(f\"Best CV Score: {result['best_score']:.3f}\")\n",
    "    print(f\"MCC: {result['mcc']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Finding best traits using f-scpre'''\n",
    "\n",
    "'''# Finding most predictive features with respect to target value (oxygen)\n",
    "select_k_best = SelectKBest(f_classif, k=10)  \n",
    "X_new = select_k_best.fit_transform(X_aligned, Y_aligned)\n",
    "selected_features = select_k_best.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected KO terms\n",
    "selected_ko_terms = X_aligned.columns[selected_features]\n",
    "\n",
    "# Fitting select K best to find the scores\n",
    "select_k_best.fit(X_aligned, Y_aligned)\n",
    "scores = select_k_best.scores_[selected_features] # scores of selected features\n",
    "\n",
    "# Sorting features by their scores\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "sorted_scores = scores[sorted_indices]\n",
    "sorted_features = selected_ko_terms[sorted_indices]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner (F score):\")\n",
    "print(sorted_features)\n",
    "\n",
    "#Plot for visualizing distribution and importance of best traits\n",
    "\n",
    "# Translate selected KO terms to their descriptions\n",
    "selected_ko_terms_list = list(sorted_features)  \n",
    "ko_descriptions_mapping = translate_ko_terms(selected_ko_terms_list)\n",
    "\n",
    "\n",
    "# Replace KO terms with their descriptions for plotting\n",
    "translated_sorted_features = [ko_descriptions_mapping[ko] for ko in sorted_features]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(translated_sorted_features)), sorted_scores)\n",
    "plt.xticks(range(len(translated_sorted_features)), translated_sorted_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('F-scores')\n",
    "plt.title('Top 50 KO Descriptions by F-score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model based feature importance score '''\n",
    "\n",
    "#Fit a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their importances\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices = sorted_indices[:10]\n",
    "selected_features = X_aligned.columns[top_k_indices]\n",
    "\n",
    "# Scores of selected features\n",
    "sorted_scores = feature_importances[top_k_indices]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner based on randomforest classifier:\")\n",
    "print(\"This is model based feature importance (uses models own feature importance score to evaluate most important ones)\")\n",
    "print(selected_features)\n",
    "\n",
    "#Translate selected KO terms to their descriptions\n",
    "translated_sorted_features = translate_ko_terms(list(selected_features))\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels = [translated_sorted_features[ko] for ko in selected_features]\n",
    "sorted_labels = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels)), sorted_scores)\n",
    "plt.xticks(range(len(sorted_labels)), selected_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('Importance Scores')\n",
    "plt.title('Top 10 KO Descriptions for Random forests')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "''' For logsitc regression'''\n",
    "\n",
    "#Fit a LogisticRegression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their coefficients\n",
    "lr_coefficients = lr_model.coef_[0]\n",
    "sorted_indices_lr = np.argsort(np.abs(lr_coefficients))[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices_lr = sorted_indices_lr[:10]\n",
    "selected_features_lr = X_aligned.columns[top_k_indices_lr]\n",
    "\n",
    "# Coefficients (importance) of selected features\n",
    "sorted_scores_lr = lr_coefficients[top_k_indices_lr]\n",
    "\n",
    "print(\"Most selected KO terms for predicting oxygen trait in hierchical manner based on logistic regression:\")\n",
    "print(\"This is model based feature importance (uses model's coefficients as importance scores)\")\n",
    "print(selected_features_lr)\n",
    "\n",
    "#Translate selected KO terms to their descriptions\n",
    "translated_sorted_features = translate_ko_terms(list(selected_features_lr))\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels_lr = [translated_sorted_features[ko] for ko in selected_features_lr]\n",
    "sorted_labels_lr = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels_lr)), np.abs(sorted_scores_lr))  # Use absolute value for visualization\n",
    "plt.xticks(range(len(sorted_labels_lr)), translated_sorted_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('KO Descriptions')\n",
    "plt.ylabel('Coefficient Magnitudes')\n",
    "plt.title('Top 10 KO Descriptions for Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance for each trophic level\n",
    "for trophic in trophic_levels:\n",
    "    print(f\"\\nFeature importance for {trophic}:\")\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_aligned, Y_aligned[trophic].values)\n",
    "    \n",
    "    feature_importances = model.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    top_k_indices = sorted_indices[:10]\n",
    "    selected_features = X_aligned.columns[top_k_indices]\n",
    "    \n",
    "    sorted_scores = feature_importances[top_k_indices]\n",
    "    \n",
    "    print(f\"Top KO terms for {trophic}:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    translated_sorted_features = translate_ko_terms(list(selected_features))\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.bar(range(len(selected_features)), sorted_scores)\n",
    "    plt.xticks(range(len(selected_features)), translated_sorted_features, rotation='vertical', fontsize=8)\n",
    "    plt.xlabel('KO Descriptions')\n",
    "    plt.ylabel('Importance Scores')\n",
    "    plt.title(f'Top 10 KO Descriptions for {trophic}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg = KEGG()\n",
    "\n",
    "def translate_ko_terms(ko_terms):\n",
    "    ko_descriptions = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            result = kegg.find(\"ko\", ko)\n",
    "            # The result can contain multiple lines if the term has multiple entries; split by newline\n",
    "            first_line = result.split('\\n')[0]\n",
    "            # Each line is tab-separated with the format: entry_id, description\n",
    "            _, description = first_line.split('\\t', 1)\n",
    "            ko_descriptions[ko] = description\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving information for {ko}: {e}\")\n",
    "            ko_descriptions[ko] = ko  # Use the KO term itself if the name can't be retrieved\n",
    "    return ko_descriptions\n",
    "\n",
    "def map_ko_to_pathways(ko_terms):\n",
    "    pathways = {}\n",
    "    for ko in ko_terms:\n",
    "        try:\n",
    "            gene_links = kegg.link(\"pathway\", ko)\n",
    "            if gene_links:\n",
    "                for entry in gene_links.strip().split(\"\\n\"):\n",
    "                    split_entry = entry.split(\"\\t\")\n",
    "                    if len(split_entry) >= 2:\n",
    "                        ko_id, pathway_id = split_entry[0], split_entry[1]\n",
    "                        if pathway_id not in pathways:\n",
    "                            pathways[pathway_id] = set()\n",
    "                        pathways[pathway_id].add(ko)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ko}: {e}\")\n",
    "    return pathways\n",
    "\n",
    "selected_important_features = selected_features_lr\n",
    "pathways = map_ko_to_pathways(selected_important_features)\n",
    "translated_kos = translate_ko_terms(selected_important_features)  \n",
    "\n",
    "# Creating the adjacency matrix with translated KO terms, including original KO term\n",
    "pathway_matrix = pd.DataFrame(\n",
    "    index=[f\"{translated_kos[ko]} ({ko})\" for ko in selected_important_features],\n",
    "    columns=pathways.keys(),\n",
    "    data=0\n",
    ")\n",
    "for pathway, kos in pathways.items():\n",
    "    for ko in kos:\n",
    "        if ko in selected_important_features:\n",
    "            pathway_matrix.loc[f\"{translated_kos[ko]} ({ko})\", pathway] = 1\n",
    "\n",
    "# Fetch and rename pathway names for readability\n",
    "pathway_names = {}\n",
    "for column in pathway_matrix.columns:\n",
    "    pathway_info = kegg.get(column)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else column\n",
    "    pathway_names[column] = pathway_name\n",
    "    pathway_matrix.rename(columns={column: pathway_name}, inplace=True)\n",
    "\n",
    "print(\"Pathway matrix after renaming:\\n\", pathway_matrix)\n",
    "\n",
    "# Heatmap visualization\n",
    "sns.heatmap(pathway_matrix, annot=True, cmap=\"Greys\", cbar=False)\n",
    "plt.title('Adjacency Matrix of KO Terms and Pathways LR')\n",
    "plt.xlabel('Pathways')\n",
    "plt.ylabel('KO Terms')\n",
    "plt.show()\n",
    "\n",
    "# Initialize a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define a list of general pathways to exclude\n",
    "excluded_pathways = [\"metabolic pathways\"]  # You can add more general terms here\n",
    "\n",
    "# Add nodes and edges with renamed pathway names\n",
    "for ko in selected_important_features:\n",
    "    translated_label = f\"{translated_kos[ko]} ({ko})\"\n",
    "    G.add_node(ko, title=translated_label, label=translated_label, color='red', size=20)\n",
    "\n",
    "for pathway_id, kos in pathways.items():\n",
    "    pathway_info = kegg.get(pathway_id)\n",
    "    parsed_info = kegg.parse(pathway_info)\n",
    "    pathway_name = parsed_info['NAME'][0] if 'NAME' in parsed_info else pathway_id\n",
    "    if pathway_name.lower() not in excluded_pathways:\n",
    "        G.add_node(pathway_name, title=pathway_name, label=pathway_name, color='blue', size=30)\n",
    "        for ko in kos:\n",
    "            G.add_edge(ko, pathway_name)\n",
    "\n",
    "# Pyvis network visualization\n",
    "nt = Network(\"800px\", \"1200px\", notebook=True, heading='Interactive Network of KO Terms and Pathways LR', bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources='remote')\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "\n",
    "nt.save_graph(\"ko_network_lr.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' TROUBLE SHOOTING '''\n",
    "\n",
    "# Check the distribution of classes in Y_train\n",
    "class_distribution = pd.Series(Y_train).value_counts()\n",
    "print(\"Class distribution in Y_train:\\n\", class_distribution)\n",
    "\n",
    "# Check unique labels in Y_train\n",
    "unique_labels = np.unique(Y_train)\n",
    "print(f\"Unique labels in Y_train: {unique_labels}\")\n",
    "\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform Y_train to encode labels\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "# Replace Y_train with the encoded labels if necessary\n",
    "Y_train = Y_train_encoded\n",
    "\n",
    "# Now, you can inspect the unique values of the encoded Y_train\n",
    "print(f\"Unique encoded labels in Y_train: {np.unique(Y_train)}\")\n",
    "\n",
    "\n",
    "# Check if the number of instances matches\n",
    "print(f\"Number of instances in X_train: {X_train.shape[0]}\")\n",
    "print(f\"Number of labels in Y_train: {len(Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Comparing F1 and MCC plots'''\n",
    "\n",
    "# Define the range of `k` values to explore\n",
    "k_values = range(1, 1000, 20)  # Range of steps\n",
    "\n",
    "# Define estimators to compare\n",
    "estimators = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'SupportVectorMachines': SVC(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'BernoulliNB': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Prepare to store results for both F1 and MCC\n",
    "results = {name: {'f1': [], 'mcc': []} for name in estimators}\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Loop over each estimator\n",
    "for name, estimator in estimators.items():\n",
    "    print(f\"Processing estimator: {name}\")  # Debug print\n",
    "    # Loop over each `k` value\n",
    "    for k in k_values:\n",
    "        print(f\"Testing with k={k}\")  # Debug print\n",
    "        # Define the pipeline for the current estimator\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif, k=k)),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation for F1-score\n",
    "        f1_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "        results[name]['f1'].append(f1_scores.mean())\n",
    "        \n",
    "        # Perform cross-validation for MCC\n",
    "        mcc_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(matthews_corrcoef), n_jobs=-1)\n",
    "        results[name]['mcc'].append(mcc_scores.mean())\n",
    "\n",
    "# For debugging \n",
    "print(results)\n",
    "\n",
    "''' Plotting '''\n",
    "\n",
    "# Plotting the results for each estimator for both F1 and MCC\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    k_values_list = list(k_values)  # Convert range to list for indexing\n",
    "    # F1 Plot\n",
    "    finite_f1_scores = [score for score in scores['f1'] if np.isfinite(score)]\n",
    "    finite_k_values_f1 = [k for k, score in zip(k_values_list, scores['f1']) if np.isfinite(score)]\n",
    "    ax[0].plot(finite_k_values_f1, finite_f1_scores, marker='o', linestyle='-', label=name)\n",
    "    \n",
    "    # highest F1 score\n",
    "    if finite_f1_scores:  # Check if there are any finite scores\n",
    "        max_f1_score = max(finite_f1_scores)\n",
    "        max_f1_index = finite_f1_scores.index(max_f1_score)\n",
    "        best_k_f1 = finite_k_values_f1[max_f1_index]\n",
    "        ax[0].annotate(f'{max_f1_score:.2f}', (best_k_f1, max_f1_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # MCC Plot\n",
    "    finite_mcc_scores = [score for score in scores['mcc'] if np.isfinite(score)]\n",
    "    finite_k_values_mcc = [k for k, score in zip(k_values_list, scores['mcc']) if np.isfinite(score)]\n",
    "    ax[1].plot(finite_k_values_mcc, finite_mcc_scores, marker='o', linestyle='-', label=name)\n",
    "\n",
    "    #  highest MCC score\n",
    "    if finite_mcc_scores:  # Check if there are any finite scores\n",
    "        max_mcc_score = max(finite_mcc_scores)\n",
    "        max_mcc_index = finite_mcc_scores.index(max_mcc_score)\n",
    "        best_k_mcc = finite_k_values_mcc[max_mcc_index]\n",
    "        ax[1].annotate(f'{max_mcc_score:.2f}', (best_k_mcc, max_mcc_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    \n",
    "\n",
    "ax[0].set_title('F1 Score by Number of Selected Features (k) for Different Estimators')\n",
    "ax[0].set_xlabel('Number of Features (k)')\n",
    "ax[0].set_ylabel('F1 Score')\n",
    "\n",
    "ax[1].set_title('MCC by Number of Selected Features (k) for Different Estimators')\n",
    "ax[1].set_xlabel('Number of Features (k)')\n",
    "ax[1].set_ylabel('MCC Score')\n",
    "\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot for LR vs RFS\n",
    "\n",
    "# Fit models\n",
    "lr_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting to aligned matrixes\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "# Get coefficients and feature importances\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "sorted_indices_rf = np.argsort(rf_feature_importance)[::-1]\n",
    "sorted_indices_lr = np.argsort(logistic_importance)[::-1]\n",
    "\n",
    "\n",
    "top_k_indices_rf = sorted_indices_rf[:10]\n",
    "top_k_indices_lr = sorted_indices_lr[:10]\n",
    "selected_features_rf = X_aligned.columns[top_k_indices_rf]\n",
    "selected_features_lr = X_aligned.columns[top_k_indices_lr]\n",
    "\n",
    "print(selected_features_rf)\n",
    "print(selected_features_lr)\n",
    "\n",
    "sorted_scores_rf = rf_feature_importance[top_k_indices_rf]\n",
    "sorted_scores_lr = logistic_importance[top_k_indices_lr]\n",
    "\n",
    "\n",
    "# Normalize the importance scores\n",
    "scaler = MinMaxScaler()\n",
    "lr_importance_scaled = scaler.fit_transform(logistic_importance.reshape(-1, 1)).flatten()\n",
    "rf_importance_scaled = scaler.fit_transform(rf_feature_importance.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(lr_importance_scaled, rf_importance_scaled, alpha=0.6, color='red')\n",
    "\n",
    "# Line of best fit\n",
    "m, b = np.polyfit(rf_importance_scaled, lr_importance_scaled, 1)\n",
    "ax.plot(rf_importance_scaled, m*rf_importance_scaled + b, color='blue')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Logistic Regression Importance Scores (LR)')\n",
    "ax.set_ylabel('Random Forest Importance Scores (RFS)')\n",
    "ax.set_title('Comparison of Feature Importance Scores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Important to remember\n",
    "\n",
    "''' \n",
    "\n",
    "#Logistic Regression Coefficients: After normalizing, they represent the relative predictive strength of features, \n",
    "#but they lose their direct probabilistic interpretation.\n",
    "\n",
    "#Random Forest Feature Importances: They indicate how much each feature contributes to the predictive accuracy of the model, \n",
    "#based on how much each feature decreases the impurity of the splits.\n",
    "\n",
    "#Maybe test rankings of features instead of normalized values. Can compare and see which features are consistently considered\n",
    "#important across both models\n",
    "'''\n",
    "\n",
    "\n",
    "# Rank the features by importance for each model\n",
    "lr_ranking = np.argsort(logistic_importance)[::-1]  # argsort returns indices that would sort the array\n",
    "rf_ranking = np.argsort(rf_feature_importance)[::-1]\n",
    "\n",
    "\n",
    "# For visualization\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = X_aligned.columns  # For feature names\n",
    "ranking_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'LR_Rank': lr_ranking.argsort(),\n",
    "    'RF_Rank': rf_ranking.argsort(),\n",
    "    'LR_Score': logistic_importance[lr_ranking.argsort()],\n",
    "    'RF_Score': rf_feature_importance[rf_ranking.argsort()]\n",
    "})\n",
    "\n",
    "\n",
    "# Select top features by rank threshold or specific indices if needed\n",
    "top_n = 10\n",
    "ranking_df['Top_LR'] = ranking_df['LR_Rank'] < top_n\n",
    "ranking_df['Top_RF'] = ranking_df['RF_Rank'] < top_n\n",
    "\n",
    "# Now print or use this DataFrame for visualization\n",
    "print(ranking_df.head(20))  # Display the first 20 entries for checking\n",
    "\n",
    "''' \n",
    "# Sorts based on logisitc regression rank\n",
    "#ranking_df.sort_values('LR_Rank', inplace=True)\n",
    "# Can also sort by 'RF_Rank' \n",
    "# Ranks from random forests\n",
    "#print(ranking_df[['Feature', 'LR_Rank', 'RF_Rank']].head(100))\n",
    "# This shows the top 20 features as ranked by logistic regression, alongside their ranks in random forest\n",
    "'''\n",
    "\n",
    "# To find features that are consistently considered important, look for low rank numbers in both models\n",
    "consistently_important_features = ranking_df[(ranking_df['LR_Rank'] < 100) & (ranking_df['RF_Rank'] < 100)]\n",
    "print(consistently_important_features)\n",
    "\n",
    "\n",
    "\n",
    "\"importance\" in logistic regression is based on the magnitude of the coefficients (which reflect the odds of the outcome), \n",
    "#while in random forests, importance is derived from how much a feature decreases the impurity of the splits.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_aligned and Y_aligned are defined and appropriate for model fitting\n",
    "\n",
    "# Fit models\n",
    "lr_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting to aligned matrices\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "# Get coefficients and feature importances\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Rank the features by importance for each model\n",
    "lr_ranking = np.argsort(logistic_importance)[::-1]  # argsort returns indices that would sort the array\n",
    "rf_ranking = np.argsort(rf_feature_importance)[::-1]\n",
    "\n",
    "# Normalize the importance scores\n",
    "scaler = MinMaxScaler()\n",
    "lr_importance_scaled = scaler.fit_transform(logistic_importance.reshape(-1, 1)).flatten()\n",
    "rf_importance_scaled = scaler.fit_transform(rf_feature_importance.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(lr_importance_scaled, rf_importance_scaled, alpha=0.6, color='red')\n",
    "\n",
    "# Line of best fit\n",
    "m, b = np.polyfit(rf_importance_scaled, lr_importance_scaled, 1)\n",
    "ax.plot(rf_importance_scaled, m*rf_importance_scaled + b, color='blue')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Logistic Regression Importance Scores (LR)')\n",
    "ax.set_ylabel('Random Forest Importance Scores (RFS)')\n",
    "ax.set_title('Comparison of Feature Importance Scores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For visualization\n",
    "feature_names = X_aligned.columns  # Assuming your DataFrame columns are the feature names\n",
    "ranking_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'LR_Rank': lr_ranking.argsort(),  # Convert indices to rank positions\n",
    "    'RF_Rank': rf_ranking.argsort(),\n",
    "    'LR_Score': logistic_importance[lr_ranking.argsort()],  # Order scores by original feature order\n",
    "    'RF_Score': rf_feature_importance[rf_ranking.argsort()]\n",
    "})\n",
    "\n",
    "# Sort DataFrame based on what is needed\n",
    "ranking_df.sort_values('RF_Rank', ascending=True, inplace=True)\n",
    "\n",
    "# Select top features by rank threshold or specific indices if needed\n",
    "top_n = 10\n",
    "ranking_df['Top_LR'] = ranking_df['LR_Rank'] < top_n\n",
    "ranking_df['Top_RF'] = ranking_df['RF_Rank'] < top_n\n",
    "\n",
    "# Display the first 20 entries for checking\n",
    "print(ranking_df.head(20))\n",
    "\n",
    "# To find features that are consistently considered important, look for low rank numbers in both models\n",
    "consistently_important_features = ranking_df[(ranking_df['LR_Rank'] < 100) & (ranking_df['RF_Rank'] < 100)]\n",
    "print(consistently_important_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pearson R for correlation coefficient between feature importance scores (From both models)'''\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(solver='lbfgs', penalty=\"l2\")\n",
    "rf_model = RandomForestClassifier(n_estimators=500, max_depth=100, min_samples_split=5, random_state=42)\n",
    "\n",
    "\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "rf_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "\n",
    "logistic_importance = np.abs(lr_model.coef_[0])\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr, p_value = pearsonr(logistic_importance, rf_feature_importance)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr:.3f}\")\n",
    "#print(f\"P-Value: {p_value:.3f}\")\n",
    "\n",
    "# Bootstrap Pearson correlation coefficients\n",
    "n_bootstraps = 10000\n",
    "corr_coefficients = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Sample indices with replacement\n",
    "    indices = resample(np.arange(len(logistic_importance)))\n",
    "    \n",
    "    # Calculate Pearson correlation for the sample\n",
    "    r, _ = pearsonr(logistic_importance[indices], rf_feature_importance[indices])\n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=30, color='red', edgecolor='black')\n",
    "plt.title('Bootstrap Dist. of Pearson Correlation Coefficients')\n",
    "plt.xlabel('Pearson Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments on the plot:\n",
    "\n",
    "Bootstrapping was done to simulate calculating pearsons with multiple variables against a single response variable. The bootstrappiing segments the data to generate a distribution of pearson coeffiecients\n",
    "\n",
    "Bootstrapping randomly samples the data with replacemnet, calculating pearsons for each sample and reapting this many times for the distribution\n",
    "\n",
    "Gives a sense os how the relationship between LR and RF mighjt vary across different random samples of the data\n",
    "\n",
    "Pvalue is 0.00???\n",
    "\n",
    "The weak but positive relationship suggests that when features are considered important in one model, this also seems to be the case in the other.\n",
    "However, degree of importance does not seem to be aligning\n",
    "\n",
    "Positive but not strong relationship.\n",
    "\n",
    "Logisitic regression may be more sensitive to linear relationships while randomforestes may capture none-linear dependancies better.\n",
    "\n",
    "Pearson correlation coefficent is invariant, but the way the feature impact the model output probably varies a lot. (If feature have different scales, which i think they do, at least the importance)\n",
    "\n",
    "LR gives negative or positive coefficents, refelcting the direction and strength of the relationship with the dependant variable\n",
    "RF uses a measure based on how much each feature decreases the impurity of the splits. This wont nesecarilly imply direction, only the usefullness or utility of the feature in node splitting\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
