{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c9589ae-4e09-4db5-ab9a-3c75c48785c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Data_Feature')\n",
    "sys.path.append('../Datasets')\n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB # or GaussianNB if your data is normalized and continuous\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, make_scorer, matthews_corrcoef, roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import plotly.figure_factory as ff\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from pyvis.network import Network\n",
    "from bioservices import KEGG\n",
    "import zipfile \n",
    "\n",
    "\n",
    "'''\n",
    "Data prep and feature mapping\n",
    "'''\n",
    "\n",
    "os.chdir('../Datasets')\n",
    "# Paths to your uploaded files\n",
    "zip_file_path_KO = \"terms_KO.zip\"\n",
    "csv_file_name_KO = 'terms_KO.csv'\n",
    "zip_file_path_traits = 'reducedDataset.zip'\n",
    "csv_file_name_traits = 'reducedDataset.csv'\n",
    "# Read and preprocess KO terms\n",
    "with zipfile.ZipFile(zip_file_path_KO, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_KO) as file:\n",
    "        KOs = pd.read_csv(file, index_col=0)\n",
    "\n",
    "KOs['value'] = 1\n",
    "X_terms = KOs.pivot_table(index='key', columns='KO', values='value', fill_value=0)\n",
    "\n",
    "# Apply VarianceThreshold to remove constant features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_filtered = selector.fit_transform(X_terms)\n",
    "X_filtered_df = pd.DataFrame(X_filtered, index=X_terms.index, columns=X_terms.columns[selector.get_support()])\n",
    "\n",
    "# Optionally, inspect the variance of features to confirm removal\n",
    "feature_variances = X_filtered_df.var()\n",
    "print(f\"Features with zero variance after filtering: {sum(feature_variances == 0)}\")\n",
    "\n",
    "# Read and preprocess traits data\n",
    "with zipfile.ZipFile(zip_file_path_traits, 'r') as zip_ref:\n",
    "    with zip_ref.open(csv_file_name_traits) as file:\n",
    "        traits = pd.read_csv(file, sep=';')\n",
    "traits['oxygen'] = traits['oxygen'].str.lower()\n",
    "\n",
    "# Mapping of oxygen levels to a simplified classification\n",
    "oxygen_mapping = {\n",
    "    'aerobic': 'aerobic',\n",
    "    'aerotolerant': 'aerobic',\n",
    "    'microaerophilic': 'aerobic',\n",
    "    'obligate aerobic': 'aerobic',\n",
    "    'anaerobic': 'anaerobic',\n",
    "    'obligate anaerobic': 'anaerobic',\n",
    "    'conflict': 'aerobic',  \n",
    "    'facultative': 'aerobic'  \n",
    "}\n",
    "traits['oxygen'] = traits['oxygen'].map(oxygen_mapping)\n",
    "\n",
    "\n",
    "# Handling missing values and aggregating by key\n",
    "y = traits.dropna(subset=['oxygen']).groupby('key').agg({'oxygen': lambda x: x.value_counts().index[0]})\n",
    "\n",
    "# Find common keys after removing missing values\n",
    "common_keys = X_filtered_df.index.intersection(y.index)\n",
    "\n",
    "# Align X (features) and Y (labels) based on common keys\n",
    "X_aligned = X_filtered_df.loc[common_keys]\n",
    "Y_aligned = y.loc[common_keys].values.ravel()\n",
    "\n",
    "# Ensures X_aligned and Y_aligned are aligned\n",
    "assert X_aligned.shape[0] == len(Y_aligned), \"X and Y are not aligned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculating Pearsons coefficient'''\n",
    "\n",
    "# Always look at data, right?\n",
    "print(X_aligned.head())\n",
    "\n",
    "\n",
    "'''Handelig data values'''\n",
    "\n",
    "# Checking datatype for y_algned, i think\n",
    "class_distribution = pd.Series(Y_aligned).value_counts()\n",
    "print(\"Class distribution in Y_aligned:\", class_distribution)\n",
    "\n",
    "# Checks data label of Y algined\n",
    "unique_labels = np.unique(Y_aligned)\n",
    "print(f\"Unique labels in Y_aligned: {unique_labels}\")\n",
    "\n",
    "# Initialize the LabelEncoder and transform the data from catagorical\n",
    "label_encoder = LabelEncoder()\n",
    "Y_aligned = label_encoder.fit_transform(Y_aligned)\n",
    "\n",
    "# Check data once labelencoder is done\n",
    "print(f\"Data of Y_aligned: {np.unique(Y_aligned)}\")\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "corr_coefficients = [] # Collectes coefficents from X_aligned and Y_aligned calculations\n",
    "\n",
    "# I should probably check how pearsons actually works\n",
    "\n",
    "\n",
    "for column in X_aligned.columns: # Iterates over every column in X_aligned\n",
    "    # Easier to check all the different collumns because its a pandas data frame\n",
    "    # Interger indexing is cool but i dont like it\n",
    "    r, p = pearsonr(X_aligned[column], Y_aligned)  \n",
    "    corr_coefficients.append(r)\n",
    "\n",
    "\n",
    "# PLotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(corr_coefficients, bins=100, color='red', edgecolor='black')\n",
    "plt.title('Correlations')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43657336",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pipeline implementation for Naive bayes and random forests'''\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_aligned, Y_aligned, test_size=0.3, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('select_k', SelectKBest(f_classif)),\n",
    "    ('estimator', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_k__k': [2],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [RandomForestClassifier(random_state=42)],\n",
    "        'estimator__n_estimators': [100, 200],  \n",
    "        'estimator__max_depth': [5, 10, None]  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k': [2],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [SVC(random_state=42)],\n",
    "        'estimator__C': [0.1, 1, 10],  \n",
    "        'estimator__kernel': ['linear', 'rbf'], \n",
    "        'estimator__gamma': ['scale', 'auto']  \n",
    "    },\n",
    "    {\n",
    "        'select_k__k': [2],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [LogisticRegression()],\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },         \n",
    "    {\n",
    "        'select_k__k': [2],  # May need adjusting  100, 200, 300\n",
    "        'estimator': [BernoulliNB()],\n",
    "        'estimator__alpha': [0.01, 0.1, 1.0, 10.0],  \n",
    "        'estimator__binarize': [0.0]  # Is automatically applied\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "''' Grid search '''\n",
    "\n",
    "# Set up GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "'''MCC shenanigans'''\n",
    "\n",
    "mcc = matthews_corrcoef(Y_test, Y_pred)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc}\")\n",
    "\n",
    "\n",
    "# Display confusion matrix for the best model\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title(f'Confusion Matrix for the Best Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model based feature importance score '''\n",
    "\n",
    "#Fit a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their importances\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices = sorted_indices[:10]\n",
    "selected_features = X_aligned.columns[top_k_indices]\n",
    "\n",
    "# Scores of selected features\n",
    "sorted_scores = feature_importances[top_k_indices]\n",
    "\n",
    "print(\"Most selected GO terms for predicting oxygen trait in hierchical manner based on randomforest classifier:\")\n",
    "print(\"This is model based feature importance (uses models own feature importance score to evaluate most important ones)\")\n",
    "print(selected_features)\n",
    "\n",
    "#Translate selected GO terms to their descriptions\n",
    "# Need to find translation thingy\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels = selected_features\n",
    "sorted_labels = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels)), sorted_scores)\n",
    "plt.xticks(range(len(sorted_labels)), selected_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('GO Descriptions')\n",
    "plt.ylabel('Importance Scores')\n",
    "plt.title('Top 10 GO Descriptions for Random forests')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "''' For logsitc regression'''\n",
    "\n",
    "#Fit a LogisticRegression model\n",
    "lr_model = LogisticRegression(solver=\"liblinear\")\n",
    "lr_model.fit(X_aligned, Y_aligned)\n",
    "\n",
    "#Rank features by their coefficients\n",
    "lr_coefficients = lr_model.coef_[0]\n",
    "sorted_indices_lr = np.argsort(np.abs(lr_coefficients))[::-1]\n",
    "\n",
    "# Select the top 10 features\n",
    "top_k_indices_lr = sorted_indices_lr[:10]\n",
    "selected_features_lr = X_aligned.columns[top_k_indices_lr]\n",
    "\n",
    "# Coefficients (importance) of selected features\n",
    "sorted_scores_lr = lr_coefficients[top_k_indices_lr]\n",
    "\n",
    "print(\"Most selected GO terms for predicting oxygen trait in hierchical manner based on logistic regression:\")\n",
    "print(\"This is model based feature importance (uses model's coefficients as importance scores)\")\n",
    "print(selected_features_lr)\n",
    "\n",
    "#Translate selected KO terms to their descriptions\n",
    "\n",
    "\n",
    "# Prepare labels and scores for plotting\n",
    "labels_lr = selected_features_lr\n",
    "sorted_labels_lr = [labels[idx] for idx in range(len(labels))]\n",
    "\n",
    "# Plotting with translated names\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(range(len(sorted_labels_lr)), np.abs(sorted_scores_lr))  # Use absolute value for visualization\n",
    "plt.xticks(range(len(sorted_labels_lr)), selected_features, rotation='vertical', fontsize=8)\n",
    "plt.xlabel('GO Descriptions')\n",
    "plt.ylabel('Coefficient Magnitudes')\n",
    "plt.title('Top 10 GO Descriptions for Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' TROUBLE SHOOTING '''\n",
    "\n",
    "# Check the distribution of classes in Y_train\n",
    "class_distribution = pd.Series(Y_train).value_counts()\n",
    "print(\"Class distribution in Y_train:\\n\", class_distribution)\n",
    "\n",
    "# Check unique labels in Y_train\n",
    "unique_labels = np.unique(Y_train)\n",
    "print(f\"Unique labels in Y_train: {unique_labels}\")\n",
    "\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform Y_train to encode labels\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "# Replace Y_train with the encoded labels if necessary\n",
    "Y_train = Y_train_encoded\n",
    "\n",
    "# Now, you can inspect the unique values of the encoded Y_train\n",
    "print(f\"Unique encoded labels in Y_train: {np.unique(Y_train)}\")\n",
    "\n",
    "\n",
    "# Check if the number of instances matches\n",
    "print(f\"Number of instances in X_train: {X_train.shape[0]}\")\n",
    "print(f\"Number of labels in Y_train: {len(Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Comparing F1 and MCC plots'''\n",
    "\n",
    "# Define the range of `k` values to explore\n",
    "k_values = range(1, 1000, 20)  # Range of steps\n",
    "\n",
    "# Define estimators to compare\n",
    "estimators = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'SupportVectorMachines': SVC(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'BernoulliNB': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Prepare to store results for both F1 and MCC\n",
    "results = {name: {'f1': [], 'mcc': []} for name in estimators}\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Loop over each estimator\n",
    "for name, estimator in estimators.items():\n",
    "    print(f\"Processing estimator: {name}\")  # Debug print\n",
    "    # Loop over each `k` value\n",
    "    for k in k_values:\n",
    "        print(f\"Testing with k={k}\")  # Debug print\n",
    "        # Define the pipeline for the current estimator\n",
    "        pipeline = Pipeline([\n",
    "            ('select_k', SelectKBest(f_classif, k=k)),\n",
    "            ('estimator', estimator)\n",
    "        ])\n",
    "        \n",
    "        # Perform cross-validation for F1-score\n",
    "        f1_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "        results[name]['f1'].append(f1_scores.mean())\n",
    "        \n",
    "        # Perform cross-validation for MCC\n",
    "        mcc_scores = cross_val_score(pipeline, X_train, Y_train, cv=cv, scoring=make_scorer(matthews_corrcoef), n_jobs=-1)\n",
    "        results[name]['mcc'].append(mcc_scores.mean())\n",
    "\n",
    "# For debugging \n",
    "print(results)\n",
    "\n",
    "''' Plotting '''\n",
    "\n",
    "# Plotting the results for each estimator for both F1 and MCC\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    k_values_list = list(k_values)  # Convert range to list for indexing\n",
    "    # F1 Plot\n",
    "    finite_f1_scores = [score for score in scores['f1'] if np.isfinite(score)]\n",
    "    finite_k_values_f1 = [k for k, score in zip(k_values_list, scores['f1']) if np.isfinite(score)]\n",
    "    ax[0].plot(finite_k_values_f1, finite_f1_scores, marker='o', linestyle='-', label=name)\n",
    "    \n",
    "    # highest F1 score\n",
    "    if finite_f1_scores:  # Check if there are any finite scores\n",
    "        max_f1_score = max(finite_f1_scores)\n",
    "        max_f1_index = finite_f1_scores.index(max_f1_score)\n",
    "        best_k_f1 = finite_k_values_f1[max_f1_index]\n",
    "        ax[0].annotate(f'{max_f1_score:.2f}', (best_k_f1, max_f1_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # MCC Plot\n",
    "    finite_mcc_scores = [score for score in scores['mcc'] if np.isfinite(score)]\n",
    "    finite_k_values_mcc = [k for k, score in zip(k_values_list, scores['mcc']) if np.isfinite(score)]\n",
    "    ax[1].plot(finite_k_values_mcc, finite_mcc_scores, marker='o', linestyle='-', label=name)\n",
    "\n",
    "    #  highest MCC score\n",
    "    if finite_mcc_scores:  # Check if there are any finite scores\n",
    "        max_mcc_score = max(finite_mcc_scores)\n",
    "        max_mcc_index = finite_mcc_scores.index(max_mcc_score)\n",
    "        best_k_mcc = finite_k_values_mcc[max_mcc_index]\n",
    "        ax[1].annotate(f'{max_mcc_score:.2f}', (best_k_mcc, max_mcc_score), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    \n",
    "\n",
    "ax[0].set_title('F1 Score by Number of Selected Features (k) for Different Estimators')\n",
    "ax[0].set_xlabel('Number of Features (k)')\n",
    "ax[0].set_ylabel('F1 Score')\n",
    "\n",
    "ax[1].set_title('MCC by Number of Selected Features (k) for Different Estimators')\n",
    "ax[1].set_xlabel('Number of Features (k)')\n",
    "ax[1].set_ylabel('MCC Score')\n",
    "\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.grid(True)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
